{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "OH_Introduction_To_NLP_01.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cwmarris/pull-request-monitor/blob/master/OH_Introduction_To_NLP_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBW4gr9l1CTw"
      },
      "source": [
        "# Introduction to NLP 01: Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMvDz4e4ZhHY"
      },
      "source": [
        "* Author: Amy Zhuang\n",
        "* Date: July 2020"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3tSKUwN0fln"
      },
      "source": [
        "## What is NLP?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gagz458u2yml"
      },
      "source": [
        "Natural Language Processing (NLP) is a field of data science that gives the machines the ability to read, understand and derive meanings from human languages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zC3kxVz0m0g"
      },
      "source": [
        "## What are NLP's use cases?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_cEI9jj09wL"
      },
      "source": [
        "* Sentiment Analysis\n",
        "* Topic Modeling (unsupervised)\n",
        "* Named Entity Recognition (NER)\n",
        "* Part of Speach (POS)\n",
        "* Language Translation\n",
        "* Language Generation\n",
        "* Text Summarization\n",
        "* Text Classification (supervised)\n",
        "* Text Segmentation (unsupervised)\n",
        "* Speech to Text and Text to Speech\n",
        "* Chatbot\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILSuWNnnD1bP"
      },
      "source": [
        "## NLP Terminologies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12tUiwcLEEIw"
      },
      "source": [
        "* Stop Words\n",
        "* Tokenization\n",
        "* Stemming\n",
        "* Lemmatization\n",
        "* Count Vectorization\n",
        "* TF-IDF: Tf-idf stands for term frequency-inverse document frequency. The importance increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus\n",
        "TF-IDF=TF(t)*IDF(t)\n",
        " * TF: Term Frequency, which measures how frequently a term occurs in a document. Since every document is different in length, it is possible that a term would appear many more times in long documents than shorter ones. Thus, the term frequency is often divided by the document length (aka. the total number of terms in the document) as a way of normalization.\n",
        " * IDF: Inverse Document Frequency, which measures how important a term is. While computing TF, all terms are considered equally important. However it is known that certain terms, such as \"is\", \"of\", and \"that\", may appear a lot of times but have little importance. Thus we need to weigh down the frequent terms while scale up the rare ones, by computing the following.\n",
        "\n",
        "* Bag of Words\n",
        "* Word Embedding: for example, king-man+woman=queen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASn_zwEOFMug"
      },
      "source": [
        "## Hands-on Exercise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mn09U7_DfdlV"
      },
      "source": [
        "### Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_GBB0PB0F2D"
      },
      "source": [
        "text = 'HBAP students benefit from world-class instruction in courses designed by esteemed Harvard faculty and collaborate with diverse peers in highly interactive online classes. '"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fD0mQD9nMh60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "5d4008a0-2223-4a85-b1be-895cff55db3f"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('word_tokenize')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize \n",
        "STOPWORDS = set(stopwords.words('english'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Error loading word_tokenize: Package 'word_tokenize' not\n",
            "[nltk_data]     found in index\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IAGHkceMnwB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "5cbdff2d-c4db-41c0-cc48-fc4dbf2de9be"
      },
      "source": [
        "# Tokenization\n",
        "tokens = word_tokenize(text)\n",
        "tokens"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['HBAP',\n",
              " 'students',\n",
              " 'benefit',\n",
              " 'from',\n",
              " 'world-class',\n",
              " 'instruction',\n",
              " 'in',\n",
              " 'courses',\n",
              " 'designed',\n",
              " 'by',\n",
              " 'esteemed',\n",
              " 'Harvard',\n",
              " 'faculty',\n",
              " 'and',\n",
              " 'collaborate',\n",
              " 'with',\n",
              " 'diverse',\n",
              " 'peers',\n",
              " 'in',\n",
              " 'highly',\n",
              " 'interactive',\n",
              " 'online',\n",
              " 'classes',\n",
              " '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OygcIfDENsel",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "8ff40e24-1aac-48e1-e4b1-caa43fc9398a"
      },
      "source": [
        "# Remove Stopping Words\n",
        "text_no_stopwords = [w for w in tokens if not w in STOPWORDS] \n",
        "text_no_stopwords"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['HBAP',\n",
              " 'students',\n",
              " 'benefit',\n",
              " 'world-class',\n",
              " 'instruction',\n",
              " 'courses',\n",
              " 'designed',\n",
              " 'esteemed',\n",
              " 'Harvard',\n",
              " 'faculty',\n",
              " 'collaborate',\n",
              " 'diverse',\n",
              " 'peers',\n",
              " 'highly',\n",
              " 'interactive',\n",
              " 'online',\n",
              " 'classes',\n",
              " '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LxLrO5kfqeh"
      },
      "source": [
        "### Stemming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogozBQKfPkmf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "e48b32c8-2913-44d0-bebc-52222413484c"
      },
      "source": [
        "# Stemming\n",
        "from nltk.stem import PorterStemmer\n",
        "text_stemmed = [PorterStemmer().stem(w) for w in text_no_stopwords]\n",
        "text_stemmed"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hbap',\n",
              " 'student',\n",
              " 'benefit',\n",
              " 'world-class',\n",
              " 'instruct',\n",
              " 'cours',\n",
              " 'design',\n",
              " 'esteem',\n",
              " 'harvard',\n",
              " 'faculti',\n",
              " 'collabor',\n",
              " 'divers',\n",
              " 'peer',\n",
              " 'highli',\n",
              " 'interact',\n",
              " 'onlin',\n",
              " 'class',\n",
              " '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQFXPVhpft-M"
      },
      "source": [
        "### Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ASrccGqRooH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "1ec390d3-31ef-4ffe-b994-e486f389bc65"
      },
      "source": [
        "# Lemmatization\n",
        "nltk.download('wordnet')\n",
        "wn = nltk.WordNetLemmatizer()\n",
        "text_lemma = [wn.lemmatize(w) for w in text_no_stopwords]\n",
        "text_lemma"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['HBAP',\n",
              " 'student',\n",
              " 'benefit',\n",
              " 'world-class',\n",
              " 'instruction',\n",
              " 'course',\n",
              " 'designed',\n",
              " 'esteemed',\n",
              " 'Harvard',\n",
              " 'faculty',\n",
              " 'collaborate',\n",
              " 'diverse',\n",
              " 'peer',\n",
              " 'highly',\n",
              " 'interactive',\n",
              " 'online',\n",
              " 'class',\n",
              " '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqdNVSmuTCjh"
      },
      "source": [
        "Tip: Choose Stemming for speed and lemmatization for accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UefToaSjfyet"
      },
      "source": [
        "### Count Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xkln07VMVX2l"
      },
      "source": [
        "# Count Vectorization\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCNwWPORavmP"
      },
      "source": [
        "text1 = ['Data science is fun.', 'Data science helps us to make data driven decisions.']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFvjx8jOawg8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "107bf6e7-9fe0-4f18-d8b8-f1a3ca72f2a2"
      },
      "source": [
        "vectorizer.fit(text1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
              "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
              "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
              "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
              "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                tokenizer=None, vocabulary=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fJ2r_Vza3Kb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "5da0002f-d48f-469d-f5a1-b204c4596d35"
      },
      "source": [
        "print('Vocabulary: ')\n",
        "print(vectorizer.vocabulary_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary: \n",
            "{'data': 0, 'science': 7, 'is': 5, 'fun': 3, 'helps': 4, 'us': 9, 'to': 8, 'make': 6, 'driven': 2, 'decisions': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jr8cOoNFbDIZ"
      },
      "source": [
        "vector = vectorizer.transform(text1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czBfOMsGbDEg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "e43cfe1d-781d-4a6c-e76b-32a079046361"
      },
      "source": [
        "print('Full vector: ')\n",
        "print(vector.toarray())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Full vector: \n",
            "[[1 0 0 1 0 1 0 1 0 0]\n",
            " [2 1 1 0 1 0 1 1 1 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgamgYAqf2NH"
      },
      "source": [
        "### TFIDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hjos3-SSbDAN"
      },
      "source": [
        "# TFIDF Vectorization\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf = TfidfVectorizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbvuraaCbC79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "7ac75d94-5326-4472-edd4-b91d32b83601"
      },
      "source": [
        "tfidf.fit(text1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
              "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
              "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
              "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
              "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
              "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                tokenizer=None, use_idf=True, vocabulary=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCkaa-D5bC3-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "6a595f4c-9def-422b-c2b5-2cb75d2838de"
      },
      "source": [
        "print('Vocabulary: ')\n",
        "print(tfidf.vocabulary_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary: \n",
            "{'data': 0, 'science': 7, 'is': 5, 'fun': 3, 'helps': 4, 'us': 9, 'to': 8, 'make': 6, 'driven': 2, 'decisions': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcm6RrcVbC0R",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "f71b2114-a7ab-4d82-d50d-f187bf3ded8a"
      },
      "source": [
        "vector_tfidf = tfidf.transform(text1)\n",
        "print('Full vector: ')\n",
        "print(vector_tfidf.toarray())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Full vector: \n",
            "[[0.40993715 0.         0.         0.57615236 0.         0.57615236\n",
            "  0.         0.40993715 0.         0.        ]\n",
            " [0.48719673 0.342369   0.342369   0.         0.342369   0.\n",
            "  0.342369   0.24359836 0.342369   0.342369  ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KWTXhA4eGhi"
      },
      "source": [
        "## NLP Learning Materials"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdTSiB_6ey-g"
      },
      "source": [
        "* NLP with Deep Learning from Stanford: https://www.youtube.com/watch?v=8rXD5-xhemo&list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z\n",
        "* NLP with Python: https://www.udemy.com/course/nlp-natural-language-processing-with-python/\n",
        "* spaCy documentation: https://spacy.io/api/doc\n",
        "* NLTK documenation: https://www.nltk.org/\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1vBMS3DbCv7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRNRLbb0bCsN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKart2ZQbCn8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}