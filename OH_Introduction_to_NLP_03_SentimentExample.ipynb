{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "OH_Introduction_to_NLP_03_SentimentExample.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "193f603fa43b4c299a86bc3e24de3116": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d602d562e09d49179d835e1b296c47bf",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ab734aae3e3645fca760df978cd99aa5",
              "IPY_MODEL_f682f15f84194cad9740857a609e3d28"
            ]
          }
        },
        "d602d562e09d49179d835e1b296c47bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ab734aae3e3645fca760df978cd99aa5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b104b0fb9bda4fe58340d40798963586",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 213450,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 213450,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_73fe8f33a1db485d907d55a007fbc170"
          }
        },
        "f682f15f84194cad9740857a609e3d28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4c5bc6e60e0547439e9d3b4f680e097f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 213k/213k [00:00&lt;00:00, 3.02MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_402da51904b5407e9cf8c224c9a0b9d2"
          }
        },
        "b104b0fb9bda4fe58340d40798963586": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "73fe8f33a1db485d907d55a007fbc170": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4c5bc6e60e0547439e9d3b4f680e097f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "402da51904b5407e9cf8c224c9a0b9d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ea857e84ff944d82addf48eba0fd30be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_bcd0c19667c341f0ba2f796a1b36733a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_964a71cca7a84a76ab01d66d29dd4fdb",
              "IPY_MODEL_255c3f597a3e4d5db352941c683f07a0"
            ]
          }
        },
        "bcd0c19667c341f0ba2f796a1b36733a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "964a71cca7a84a76ab01d66d29dd4fdb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e23780be7e814d0fabe988c86ab33acb",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 433,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 433,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_896bdba8a78048c18de3768026a1fef2"
          }
        },
        "255c3f597a3e4d5db352941c683f07a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_153b037aa5974882aeecd9bb3a2e007c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 433/433 [00:07&lt;00:00, 57.7B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fa01f3a5cc854299b734c382f0d9e156"
          }
        },
        "e23780be7e814d0fabe988c86ab33acb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "896bdba8a78048c18de3768026a1fef2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "153b037aa5974882aeecd9bb3a2e007c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fa01f3a5cc854299b734c382f0d9e156": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cde02eb9aa5140acbb82ee62a35ebee9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4b1ac4e614484541b19cd6cd78b5489c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4fd08826843d4b00889ef25e089dc462",
              "IPY_MODEL_2c689ff09d0244859d8852893fcaee60"
            ]
          }
        },
        "4b1ac4e614484541b19cd6cd78b5489c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4fd08826843d4b00889ef25e089dc462": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1b5198ad074842ef9e02b60e2a138476",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 435779157,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 435779157,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d60c2b288bb84390956b6a4ff47a09e7"
          }
        },
        "2c689ff09d0244859d8852893fcaee60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_883bbe86f4c94c3c8c7c155f53b0f9d8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 436M/436M [00:07&lt;00:00, 60.1MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ca8e32fe69f24306bf253e39f5c086ab"
          }
        },
        "1b5198ad074842ef9e02b60e2a138476": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d60c2b288bb84390956b6a4ff47a09e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "883bbe86f4c94c3c8c7c155f53b0f9d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ca8e32fe69f24306bf253e39f5c086ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cwmarris/pull-request-monitor/blob/master/OH_Introduction_to_NLP_03_SentimentExample.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_g32uyjqacV"
      },
      "source": [
        "# Sentiment Analysis Python Example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vI1gP7HBBARs"
      },
      "source": [
        "* Author: Amy Zhuang\n",
        "* Last Updated: May 2021"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1g3ODcfSHE5"
      },
      "source": [
        "## Sentiment Analysis Types"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMeuGqhcG1Re"
      },
      "source": [
        "\n",
        "- Lexicon-based: The lexicon-based method has a dictionary of sentiment scores for each word in a sentence. The average sentiment score of each word in a sentence is used to calculate the sentiment of the whole sentence. There are two popular lexicon-based sentiment analysis python packages: TextBlob and VADER.\n",
        "- Cloud API: Some cloud service providers have sentiment analysis as a service.  \n",
        "- Machine Learning Model: Machine learning model method uses predictive model to predict the sentiment of a document. Based on whether the predictors are bag of words or sequence, either a binary classification model (e.g., logistic regression, SVM, Naive Bayesian, Random Forest etc.) or a RNN model can be used.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjCtn0b8b3eO"
      },
      "source": [
        "## Mount Google Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QW7bF3vq3KJ_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3fc25071-a411-480a-a9b9-71fa22a0b1b2"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKpkpf7x3KKE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5050d5ed-4405-46a6-9906-acdd1a2246f1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWbr00oYNQBJ"
      },
      "source": [
        "import os\n",
        "os.chdir(\"drive/My Drive/Colab Notebooks\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExCPh7ZeNfZ0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5cc53575-213c-4e82-fa3e-4fa8511a1c2e"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvLGEv863KKM"
      },
      "source": [
        "## Verify GPU and TPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qv37uRky3KKN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "386645cd-e431-46cf-e5e3-4e14d86b46b1"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPGsbMLA3KKR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1000686a-206b-4135-9210-801b7952151b"
      },
      "source": [
        "import os\n",
        "if 'COLAB_TPU_ADDR' not in os.environ:\n",
        "  print('Not connected to TPU')\n",
        "else:\n",
        "  print(\"Connected to TPU\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Not connected to TPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiIDQY1UHR_N"
      },
      "source": [
        "## Download Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Q1bBtSIHUO_"
      },
      "source": [
        "1. Go to: https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences\n",
        "2. Click \"Data Folder\"\n",
        "3. Download \"sentiment labelled sentences.zip\"\n",
        "4. Unzip \"sentiment labelled sentences.zip\"\n",
        "5. Copy the file \"amazon_cells_labelled.txt\" to Google drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQwtIk_GyK4o"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XiU3Bz1P_EV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        },
        "outputId": "cc083778-2b8f-49b7-f36c-e200e2d48d93"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2c/4e/4f1ede0fd7a36278844a277f8d53c21f88f37f3754abf76a5d6224f76d4a/transformers-3.4.0-py3-none-any.whl (1.3MB)\n",
            "\r\u001b[K     |▎                               | 10kB 20.7MB/s eta 0:00:01\r\u001b[K     |▌                               | 20kB 20.0MB/s eta 0:00:01\r\u001b[K     |▉                               | 30kB 13.8MB/s eta 0:00:01\r\u001b[K     |█                               | 40kB 9.6MB/s eta 0:00:01\r\u001b[K     |█▎                              | 51kB 8.8MB/s eta 0:00:01\r\u001b[K     |█▋                              | 61kB 9.3MB/s eta 0:00:01\r\u001b[K     |█▉                              | 71kB 8.8MB/s eta 0:00:01\r\u001b[K     |██                              | 81kB 8.6MB/s eta 0:00:01\r\u001b[K     |██▍                             | 92kB 8.9MB/s eta 0:00:01\r\u001b[K     |██▋                             | 102kB 9.3MB/s eta 0:00:01\r\u001b[K     |██▉                             | 112kB 9.3MB/s eta 0:00:01\r\u001b[K     |███▏                            | 122kB 9.3MB/s eta 0:00:01\r\u001b[K     |███▍                            | 133kB 9.3MB/s eta 0:00:01\r\u001b[K     |███▋                            | 143kB 9.3MB/s eta 0:00:01\r\u001b[K     |████                            | 153kB 9.3MB/s eta 0:00:01\r\u001b[K     |████▏                           | 163kB 9.3MB/s eta 0:00:01\r\u001b[K     |████▍                           | 174kB 9.3MB/s eta 0:00:01\r\u001b[K     |████▊                           | 184kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████                           | 194kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 204kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 215kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 225kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████                          | 235kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 245kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 256kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 266kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████                         | 276kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 286kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 296kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 307kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████                        | 317kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 327kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 337kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 348kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 358kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 368kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 378kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████                      | 389kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 399kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 409kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 419kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████                     | 430kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 440kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 450kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 460kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████                    | 471kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 481kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 491kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 501kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 512kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 522kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 532kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 542kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 552kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 563kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 573kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 583kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 593kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 604kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 614kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████                | 624kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 634kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 645kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 655kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 665kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 675kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 686kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 696kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 706kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 716kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 727kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 737kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 747kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 757kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 768kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 778kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 788kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 798kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 808kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 819kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 829kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 839kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 849kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 860kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 870kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 880kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 890kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 901kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 911kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 921kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 931kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 942kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 952kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 962kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 972kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 983kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 993kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.0MB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 1.0MB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.0MB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 1.0MB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.0MB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.1MB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.1MB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 1.1MB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 1.1MB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.1MB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.1MB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.1MB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.1MB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.1MB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.1MB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.2MB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.2MB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.2MB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.2MB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.2MB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.2MB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.2MB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.2MB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.2MB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.2MB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.3MB 9.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 51.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 49.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Collecting tokenizers==0.9.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/a5/78be1a55b2ac8d6a956f0a211d372726e2b1dd2666bb537fea9b03abd62c/tokenizers-0.9.2-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 52.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=cc81b614ac9fb96673ba8f093024379c68060c07408b9ae4c337a38077a8e9cf\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.94 tokenizers-0.9.2 transformers-3.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0wHVGnDyOGY"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout, GRU\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "from textwrap import wrap\n",
        "\n",
        "import transformers\n",
        "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "import torch\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "pd.set_option('display.max_rows', 500)\n",
        "pd.set_option('display.max_columns', 500)\n",
        "pd.set_option('display.width', 1000)\n",
        "pd.set_option('display.max_colwidth', 1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPUxmYg0yrYl"
      },
      "source": [
        "## Readin Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mx5j0rEqJYhY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98d1decd-23fb-4762-fe78-f6b97742b9c8"
      },
      "source": [
        "colnames = ['sentence','target']\n",
        "amz_review = pd.read_csv('amazon_cells_labelled.txt', sep='\\t', names=colnames)\n",
        "amz_review.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>So there is no way for me to plug it in here in the US unless I go by a converter.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Good case, Excellent value.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Great for the jawbone.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Tied to charger for conversations lasting more than 45 minutes.MAJOR PROBLEMS!!</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The mic is great.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                             sentence  target\n",
              "0  So there is no way for me to plug it in here in the US unless I go by a converter.       0\n",
              "1                                                         Good case, Excellent value.       1\n",
              "2                                                              Great for the jawbone.       1\n",
              "3     Tied to charger for conversations lasting more than 45 minutes.MAJOR PROBLEMS!!       0\n",
              "4                                                                   The mic is great.       1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwkNN13L7Jo-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14d0e543-37e9-475b-a6b9-e35232b5140b"
      },
      "source": [
        "amz_review.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 2 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   sentence  1000 non-null   object\n",
            " 1   target    1000 non-null   int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 15.8+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nj7oG1fVQewg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7ab9256-f37a-40b2-9d4c-d5386cb48652"
      },
      "source": [
        "amz_review['target'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    500\n",
              "0    500\n",
              "Name: target, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKKQK2e3HM-n"
      },
      "source": [
        "## VADER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuCJcZUMJuCU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "outputId": "88f1c903-5e76-455b-a20e-d2192a66db96"
      },
      "source": [
        "!pip3 install -U nltk"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting nltk\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/1d/d925cfb4f324ede997f6d47bea4d9babba51b49e87a767c170b77005889d/nltk-3.4.5.zip (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 2.9MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n",
            "Building wheels for collected packages: nltk\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.4.5-cp36-none-any.whl size=1449905 sha256=dc5aff3c3eeab29e0d871367a4e1d85a2d6ac22206ffec8da23ec4f3bcf77c6f\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/86/f6/68ab24c23f207c0077381a5e3904b2815136b879538a24b483\n",
            "Successfully built nltk\n",
            "Installing collected packages: nltk\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed nltk-3.4.5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "nltk"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DA-ugxtdNH6L"
      },
      "source": [
        "import nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sCfA2EiNDB0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "630e7e9c-e544-4856-c205-ab79c0097756"
      },
      "source": [
        "nltk.download('vader_lexicon')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsD0hLU2-cVf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "22988609-9f1f-4515-ad61-dd290c5a7037"
      },
      "source": [
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "sia = SentimentIntensityAnalyzer()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykRlHeAGTQ7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "0ebcd720-25d0-4d44-a535-9731b28574b1"
      },
      "source": [
        "amz_review['scores_VADER'] = amz_review['sentence'].apply(lambda s: sia.polarity_scores(s)['compound'])\n",
        "amz_review['pred_VADER'] = amz_review['scores_VADER'].apply(lambda x: 1 if x >=0 else 0)\n",
        "amz_review.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>target</th>\n",
              "      <th>scores_VADER</th>\n",
              "      <th>pred_VADER</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>So there is no way for me to plug it in here in the US unless I go by a converter.</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.3535</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Good case, Excellent value.</td>\n",
              "      <td>1</td>\n",
              "      <td>0.8402</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Great for the jawbone.</td>\n",
              "      <td>1</td>\n",
              "      <td>0.6249</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Tied to charger for conversations lasting more than 45 minutes.MAJOR PROBLEMS!!</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.6145</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The mic is great.</td>\n",
              "      <td>1</td>\n",
              "      <td>0.6249</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                             sentence  target  scores_VADER  pred_VADER\n",
              "0  So there is no way for me to plug it in here in the US unless I go by a converter.       0       -0.3535           0\n",
              "1                                                         Good case, Excellent value.       1        0.8402           1\n",
              "2                                                              Great for the jawbone.       1        0.6249           1\n",
              "3     Tied to charger for conversations lasting more than 45 minutes.MAJOR PROBLEMS!!       0       -0.6145           0\n",
              "4                                                                   The mic is great.       1        0.6249           1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmcBDHlyT4UW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "39cb0cc0-2585-40c0-cec7-145df95e7857"
      },
      "source": [
        "amz_review.tail()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>target</th>\n",
              "      <th>scores_VADER</th>\n",
              "      <th>pred_VADER</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>The screen does get smudged easily because it touches your ear and face.</td>\n",
              "      <td>0</td>\n",
              "      <td>0.3400</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>What a piece of junk.. I lose more calls on this phone.</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.4019</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>Item Does Not Match Picture.</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>The only thing that disappoint me is the infra red port (irda).</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.4019</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>You can not answer calls with the unit, never worked once!</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                     sentence  target  scores_VADER pred_VADER\n",
              "995  The screen does get smudged easily because it touches your ear and face.       0        0.3400          1\n",
              "996                   What a piece of junk.. I lose more calls on this phone.       0       -0.4019          0\n",
              "997                                              Item Does Not Match Picture.       0        0.0000          1\n",
              "998           The only thing that disappoint me is the infra red port (irda).       0       -0.4019          0\n",
              "999                You can not answer calls with the unit, never worked once!       0        0.0000          1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDL3iIZ-0Ywl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2e8faf75-5f54-4df9-abb3-3235497974fb"
      },
      "source": [
        "# Compare Actual and Predicted\n",
        "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix\n",
        "accuracy_score(amz_review['target'],amz_review['pred_VADER'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.758"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ry30_isE2FcR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "7c54981f-a62d-4fb3-977c-f5bb1348ac3f"
      },
      "source": [
        "print(classification_report(amz_review['target'],amz_review['pred_VADER']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.54      0.69       500\n",
            "           1       0.68      0.97      0.80       500\n",
            "\n",
            "    accuracy                           0.76      1000\n",
            "   macro avg       0.82      0.76      0.75      1000\n",
            "weighted avg       0.82      0.76      0.75      1000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L238RPRf2q7A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "369a9b4d-4f1c-4ec6-8ab9-05e4fbf481ab"
      },
      "source": [
        "cm = confusion_matrix(amz_review['target'],amz_review['pred_VADER'])\n",
        "cmtx = pd.DataFrame(cm, index=['true:no', 'true:yes'], columns=['pred:no', 'pred:yes'])\n",
        "print(cmtx)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "          pred:no  pred:yes\n",
            "true:no       272       228\n",
            "true:yes       14       486\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIttKVgLzDy_"
      },
      "source": [
        "## Text Blob"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jHTc8OB3g8p",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "4b1829b5-f1a8-4814-ffcf-6ebbae0dbbd7"
      },
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "amz_review['scores_TextBlob'] = amz_review['sentence'].apply(lambda s: TextBlob(s).sentiment.polarity)\n",
        "amz_review['pred_TextBlob'] = amz_review['scores_TextBlob'].apply(lambda x: 1 if x >=0 else 0)\n",
        "amz_review.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>target</th>\n",
              "      <th>scores_VADER</th>\n",
              "      <th>pred_VADER</th>\n",
              "      <th>score_TextBlob</th>\n",
              "      <th>pred_TextBlob</th>\n",
              "      <th>scores_TextBlob</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>So there is no way for me to plug it in here in the US unless I go by a converter.</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.3535</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Good case, Excellent value.</td>\n",
              "      <td>1</td>\n",
              "      <td>0.8402</td>\n",
              "      <td>1</td>\n",
              "      <td>0.850000</td>\n",
              "      <td>1</td>\n",
              "      <td>0.850000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Great for the jawbone.</td>\n",
              "      <td>1</td>\n",
              "      <td>0.6249</td>\n",
              "      <td>1</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>1</td>\n",
              "      <td>0.800000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Tied to charger for conversations lasting more than 45 minutes.MAJOR PROBLEMS!!</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.6145</td>\n",
              "      <td>0</td>\n",
              "      <td>0.390625</td>\n",
              "      <td>1</td>\n",
              "      <td>0.390625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The mic is great.</td>\n",
              "      <td>1</td>\n",
              "      <td>0.6249</td>\n",
              "      <td>1</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>1</td>\n",
              "      <td>0.800000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                             sentence  target  scores_VADER  pred_VADER  score_TextBlob  pred_TextBlob  scores_TextBlob\n",
              "0  So there is no way for me to plug it in here in the US unless I go by a converter.       0       -0.3535           0        0.000000              1         0.000000\n",
              "1                                                         Good case, Excellent value.       1        0.8402           1        0.850000              1         0.850000\n",
              "2                                                              Great for the jawbone.       1        0.6249           1        0.800000              1         0.800000\n",
              "3     Tied to charger for conversations lasting more than 45 minutes.MAJOR PROBLEMS!!       0       -0.6145           0        0.390625              1         0.390625\n",
              "4                                                                   The mic is great.       1        0.6249           1        0.800000              1         0.800000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qMO-kYYXPIt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "a7c20542-05a3-407a-9313-27bfe75b2b80"
      },
      "source": [
        "amz_review.tail()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>target</th>\n",
              "      <th>scores_VADER</th>\n",
              "      <th>pred_VADER</th>\n",
              "      <th>score_TextBlob</th>\n",
              "      <th>pred_TextBlob</th>\n",
              "      <th>scores_TextBlob</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>The screen does get smudged easily because it touches your ear and face.</td>\n",
              "      <td>0</td>\n",
              "      <td>0.3400</td>\n",
              "      <td>1</td>\n",
              "      <td>0.433333</td>\n",
              "      <td>1</td>\n",
              "      <td>0.433333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>What a piece of junk.. I lose more calls on this phone.</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.4019</td>\n",
              "      <td>0</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>1</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>Item Does Not Match Picture.</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>The only thing that disappoint me is the infra red port (irda).</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.4019</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>You can not answer calls with the unit, never worked once!</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                     sentence  target  scores_VADER  pred_VADER  score_TextBlob  pred_TextBlob  scores_TextBlob\n",
              "995  The screen does get smudged easily because it touches your ear and face.       0        0.3400           1        0.433333              1         0.433333\n",
              "996                   What a piece of junk.. I lose more calls on this phone.       0       -0.4019           0        0.500000              1         0.500000\n",
              "997                                              Item Does Not Match Picture.       0        0.0000           1        0.000000              1         0.000000\n",
              "998           The only thing that disappoint me is the infra red port (irda).       0       -0.4019           0        0.000000              1         0.000000\n",
              "999                You can not answer calls with the unit, never worked once!       0        0.0000           1        0.000000              1         0.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQzA8ovC951C",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9938ecdc-f791-48d9-99b0-50b3653a310c"
      },
      "source": [
        "# Compare Actual and Predicted\n",
        "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix\n",
        "accuracy_score(amz_review['target'],amz_review['pred_TextBlob'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.688"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nc2Twr_zBngN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "faa1bab9-c4fd-4061-c299-69de4ec267e4"
      },
      "source": [
        "print(classification_report(amz_review['target'],amz_review['pred_TextBlob']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.41      0.57       500\n",
            "           1       0.62      0.97      0.76       500\n",
            "\n",
            "    accuracy                           0.69      1000\n",
            "   macro avg       0.77      0.69      0.66      1000\n",
            "weighted avg       0.77      0.69      0.66      1000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlHRj8BgBrFu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "af6e3240-9ea8-40a5-b1ef-4a8fe3c5febd"
      },
      "source": [
        "cm = confusion_matrix(amz_review['target'],amz_review['pred_TextBlob'])\n",
        "cmtx = pd.DataFrame(cm, index=['true:no', 'true:yes'], columns=['pred:no', 'pred:yes'])\n",
        "print(cmtx)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "          pred:no  pred:yes\n",
            "true:no       204       296\n",
            "true:yes       16       484\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6YLHGWjCF5E"
      },
      "source": [
        "## Comparision in Distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbG3VCtRYNqo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "80017e2e-6c07-4145-cc58-8bf60281d8c2"
      },
      "source": [
        "plt.hist(amz_review['scores_VADER'], bins=50, alpha=0.5, label='VADER')\n",
        "plt.hist(amz_review['scores_TextBlob'], bins=50, alpha=0.5, label='TextBlob')\n",
        "plt.legend(loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwATzc34e6JZ"
      },
      "source": [
        "## Data Preprocessing for Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptA0lMIzh6bU"
      },
      "source": [
        "X, y = (amz_review['sentence'].values, amz_review['target'].values) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L43jNO1-YK0u"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtoOwSuIZkaq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e4a7c9d1-36ce-4ba4-faaa-b9103bb4a58c"
      },
      "source": [
        "# check the max length of the sentences.\n",
        "amz_review.sentence.str.len().max()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "149"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ac1WtzLmCYc-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d6503a43-bcb0-4038-807b-92a671bfd773"
      },
      "source": [
        "len(X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZL5CGV0ChH4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b153160f-86a9-466e-920d-125bbf71aec5"
      },
      "source": [
        "len(X[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "27"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGxguQilCGPG"
      },
      "source": [
        "check_len = []\n",
        "for i in range(len(X)):\n",
        "  sentence_len=len(X[i])\n",
        "  check_len.append(sentence_len)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hR-i03LVC2FP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "outputId": "f8718a5e-781a-4062-de83-4425aa6dfa29"
      },
      "source": [
        "sns.displot(check_len)\n",
        "plt.xlabel('Number of Words')\n",
        "plt.ylabel('Frequency')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(3.674999999999997, 0.5, 'Frequency')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiJDl7cJDohK"
      },
      "source": [
        "From the above graph, the x-axis is the number of words and the y-axis is the number of reviews. We can see that if we choose the maxlen=20, we can get infromation from around 840 (1000-160) number of reviews."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7tYJHWHU3Jv"
      },
      "source": [
        "tk = Tokenizer(lower = True)\n",
        "tk.fit_on_texts(X)\n",
        "X_seq = tk.texts_to_sequences(X)\n",
        "X_pad = pad_sequences(X_seq, maxlen=20, padding='post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kSA8bHCZTKC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f2a32a68-628c-4ea8-e79d-646aa1fb5428"
      },
      "source": [
        "type(X_pad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Dgxfns-Za7L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9ff5f45b-a19b-4770-d67f-12952c639316"
      },
      "source": [
        "X_pad[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([117,   5,  53, 214,  11,  47,   8, 155,   4,  19, 337,  19,   1,\n",
              "       546, 416,   2, 241, 190,   6, 812], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfZnl_QTEgkB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9e1428fa-88ed-426e-f036-3abf9042fa5b"
      },
      "source": [
        "X[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'So there is no way for me to plug it in here in the US unless I go by a converter.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtUr3bYSa31-"
      },
      "source": [
        "## Train Test Split for LSTM and GRU Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0x1t4HdVNbV"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_pad, y, test_size = 0.20, random_state = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jy062Z_xa-AS"
      },
      "source": [
        "## Build RNN LSTM Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UH79ST_ndwsa"
      },
      "source": [
        "The embedding layer represent each word using a vector. ```input_dim```is the size of the volcabulary. ```output_dim``` is the dimension of the embedding vector. ```input_length```is the length of the padded sequence.\n",
        "The LSTM layer contains multiple parallel LSTM units, structurally identical but each eventually \"learning to remember\" some different thing.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKs31OQPiFQ-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7042adb4-18fd-4bb4-ff70-01b6e05e1558"
      },
      "source": [
        "import tensorflow as tf; print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogkI4lrJJ2gh"
      },
      "source": [
        "### Model Building"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCDWJPP5Gwo_"
      },
      "source": [
        "checkpoint_path='/content/drive/My Drive/Colab Notebooks/testLSTMmodel.h5'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEpXU6Vt3Hf2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "outputId": "af625bb4-cc27-4e11-d3d1-46a26e81dac9"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(tk.word_counts.keys())+1, output_dim=128,input_length = 20))\n",
        "model.add(LSTM(units=200, dropout=0.2, recurrent_dropout=0.2)) # dropout is for the vertical inputs of x values, and recurrent_dropout is for the dropout of horizontal recurrent input.\n",
        "model.add(Dense(1,activation='sigmoid'))\n",
        "model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
        "keras_callbacks   = [\n",
        "      EarlyStopping(monitor='val_accuracy', patience=5, mode='auto', min_delta=0.0001),\n",
        "      ModelCheckpoint(checkpoint_path, monitor='val_accuracy', save_best_only=True, mode='auto')\n",
        "]\n",
        "model.fit(X_train, y_train, batch_size=50, epochs = 50, validation_split=0.2, callbacks=keras_callbacks)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "13/13 [==============================] - 3s 197ms/step - loss: 0.6970 - accuracy: 0.4781 - val_loss: 0.6985 - val_accuracy: 0.4563\n",
            "Epoch 2/50\n",
            "13/13 [==============================] - 2s 131ms/step - loss: 0.6915 - accuracy: 0.5609 - val_loss: 0.6925 - val_accuracy: 0.5063\n",
            "Epoch 3/50\n",
            "13/13 [==============================] - 2s 127ms/step - loss: 0.6229 - accuracy: 0.6687 - val_loss: 0.5941 - val_accuracy: 0.6687\n",
            "Epoch 4/50\n",
            "13/13 [==============================] - 2s 128ms/step - loss: 0.3076 - accuracy: 0.8813 - val_loss: 0.4621 - val_accuracy: 0.7688\n",
            "Epoch 5/50\n",
            "13/13 [==============================] - 2s 124ms/step - loss: 0.1420 - accuracy: 0.9484 - val_loss: 0.8594 - val_accuracy: 0.7437\n",
            "Epoch 6/50\n",
            "13/13 [==============================] - 2s 129ms/step - loss: 0.0590 - accuracy: 0.9797 - val_loss: 0.5941 - val_accuracy: 0.7875\n",
            "Epoch 7/50\n",
            "13/13 [==============================] - 2s 124ms/step - loss: 0.0221 - accuracy: 0.9969 - val_loss: 0.9273 - val_accuracy: 0.7875\n",
            "Epoch 8/50\n",
            "13/13 [==============================] - 2s 126ms/step - loss: 0.0076 - accuracy: 0.9969 - val_loss: 0.8829 - val_accuracy: 0.7812\n",
            "Epoch 9/50\n",
            "13/13 [==============================] - 2s 127ms/step - loss: 0.0049 - accuracy: 0.9984 - val_loss: 0.8934 - val_accuracy: 0.7937\n",
            "Epoch 10/50\n",
            "13/13 [==============================] - 2s 124ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 1.0406 - val_accuracy: 0.7937\n",
            "Epoch 11/50\n",
            "13/13 [==============================] - 2s 123ms/step - loss: 8.6070e-04 - accuracy: 1.0000 - val_loss: 1.3683 - val_accuracy: 0.7875\n",
            "Epoch 12/50\n",
            "13/13 [==============================] - 2s 125ms/step - loss: 3.9900e-04 - accuracy: 1.0000 - val_loss: 1.6325 - val_accuracy: 0.7688\n",
            "Epoch 13/50\n",
            "13/13 [==============================] - 2s 127ms/step - loss: 5.3630e-04 - accuracy: 1.0000 - val_loss: 1.7579 - val_accuracy: 0.7563\n",
            "Epoch 14/50\n",
            "13/13 [==============================] - 2s 124ms/step - loss: 0.0118 - accuracy: 0.9984 - val_loss: 1.4770 - val_accuracy: 0.7688\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f2be44807f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aP5eChReKDeP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "5142989a-d696-46f6-c073-aa5aa8e62565"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_5 (Embedding)      (None, 20, 128)           240512    \n",
            "_________________________________________________________________\n",
            "lstm_5 (LSTM)                (None, 200)               263200    \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 201       \n",
            "=================================================================\n",
            "Total params: 503,913\n",
            "Trainable params: 503,913\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTsFJXGiL3xk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5cf2d036-ee5e-4919-bcde-d08210d374eb"
      },
      "source": [
        "len(tk.word_counts.keys())+1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1879"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xg48Ih_YMzHa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "77e7b4d0-8ede-43a8-ea17-7b62f52e0f55"
      },
      "source": [
        "# 1st layer number of parameters: number of input_dim*output_dim\n",
        "1879*128"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "240512"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bCHNMOiL5cF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c47ad415-c2a9-4dfe-9b2a-6ed9fcb1cf7b"
      },
      "source": [
        "# 2nd LSTM layer number of parameters: 4*(output_dim_of_last_layer*units+units**2+units) \n",
        "4*(128*200+200*200+200)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "263200"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyFtujctN70O",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1f237646-7257-4bb2-fb9a-d2c1c2b7c34a"
      },
      "source": [
        "# 3rd output layer number of parameters: output_dim_of_last_layer*output_dim_of_this_layer+output_dim_of_this_layer\n",
        "200*1+1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "201"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLn4XCM9J8Kf"
      },
      "source": [
        "### Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JUVe6o3KABn"
      },
      "source": [
        "# Load the best model\n",
        "model = load_model(checkpoint_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lh9tsePhlVll",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "814aa1a9-3c0e-47cc-85c2-992e555ad9f9"
      },
      "source": [
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 80.00%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHNHpcOihBAu"
      },
      "source": [
        "## Build RNN GRU (Gated Recurrent Unit) Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAMgQOe6hyqi"
      },
      "source": [
        "The key difference between a GRU and an LSTM is that a GRU has two gates (reset and update gates) whereas an LSTM has three gates (namely input, output and forget gates)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIJe70cyLcy3"
      },
      "source": [
        "checkpoint_path='/content/drive/My Drive/Colab Notebooks/testGRUmodel.h5'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwAqxWzdhLUm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "outputId": "d9a9c5eb-b099-4661-8505-643d2fd0cd03"
      },
      "source": [
        "model_gru = Sequential()\n",
        "model_gru.add(Embedding(input_dim=len(tk.word_counts.keys())+1, output_dim=128,input_length = 20))\n",
        "model_gru.add(GRU(200))\n",
        "model_gru.add(Dense(1, activation='sigmoid'))\n",
        "model_gru.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "keras_callbacks   = [\n",
        "      EarlyStopping(monitor='val_accuracy', patience=5, mode='auto', min_delta=0.0001),\n",
        "      ModelCheckpoint(checkpoint_path, monitor='val_accuracy', save_best_only=True, mode='auto')\n",
        "]\n",
        "model_gru.fit(X_train, y_train, batch_size=50, epochs = 50,validation_split=0.2, callbacks=keras_callbacks)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "13/13 [==============================] - 1s 94ms/step - loss: 0.6959 - accuracy: 0.4938 - val_loss: 0.6947 - val_accuracy: 0.4625\n",
            "Epoch 2/50\n",
            "13/13 [==============================] - 1s 66ms/step - loss: 0.6909 - accuracy: 0.5453 - val_loss: 0.6935 - val_accuracy: 0.5063\n",
            "Epoch 3/50\n",
            "13/13 [==============================] - 1s 66ms/step - loss: 0.6854 - accuracy: 0.5984 - val_loss: 0.6902 - val_accuracy: 0.5125\n",
            "Epoch 4/50\n",
            "13/13 [==============================] - 1s 66ms/step - loss: 0.6426 - accuracy: 0.6469 - val_loss: 0.6196 - val_accuracy: 0.6375\n",
            "Epoch 5/50\n",
            "13/13 [==============================] - 1s 64ms/step - loss: 0.4878 - accuracy: 0.8391 - val_loss: 0.5885 - val_accuracy: 0.7312\n",
            "Epoch 6/50\n",
            "13/13 [==============================] - 1s 65ms/step - loss: 0.2120 - accuracy: 0.9359 - val_loss: 0.4706 - val_accuracy: 0.7875\n",
            "Epoch 7/50\n",
            "13/13 [==============================] - 1s 65ms/step - loss: 0.0751 - accuracy: 0.9828 - val_loss: 1.0210 - val_accuracy: 0.7875\n",
            "Epoch 8/50\n",
            "13/13 [==============================] - 1s 63ms/step - loss: 0.0524 - accuracy: 0.9812 - val_loss: 0.7122 - val_accuracy: 0.7812\n",
            "Epoch 9/50\n",
            "13/13 [==============================] - 1s 62ms/step - loss: 0.0121 - accuracy: 0.9953 - val_loss: 0.8121 - val_accuracy: 0.7750\n",
            "Epoch 10/50\n",
            "13/13 [==============================] - 1s 65ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.8028 - val_accuracy: 0.7937\n",
            "Epoch 11/50\n",
            "13/13 [==============================] - 1s 66ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.9452 - val_accuracy: 0.8000\n",
            "Epoch 12/50\n",
            "13/13 [==============================] - 1s 63ms/step - loss: 4.1542e-04 - accuracy: 1.0000 - val_loss: 1.0432 - val_accuracy: 0.7937\n",
            "Epoch 13/50\n",
            "13/13 [==============================] - 1s 63ms/step - loss: 2.1392e-04 - accuracy: 1.0000 - val_loss: 1.1059 - val_accuracy: 0.8000\n",
            "Epoch 14/50\n",
            "13/13 [==============================] - 1s 63ms/step - loss: 1.5624e-04 - accuracy: 1.0000 - val_loss: 1.1457 - val_accuracy: 0.7937\n",
            "Epoch 15/50\n",
            "13/13 [==============================] - 1s 63ms/step - loss: 1.3030e-04 - accuracy: 1.0000 - val_loss: 1.1817 - val_accuracy: 0.8000\n",
            "Epoch 16/50\n",
            "13/13 [==============================] - 1s 62ms/step - loss: 1.0822e-04 - accuracy: 1.0000 - val_loss: 1.2070 - val_accuracy: 0.8000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f2be70f3d30>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gC2xXL5lMigN"
      },
      "source": [
        "### Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wp4zfxKlMnbh"
      },
      "source": [
        "# Load the best model\n",
        "model = load_model(checkpoint_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BBczOhmiJEW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9ab82e87-ed49-431d-f344-a9173d8c32db"
      },
      "source": [
        "scores = model_gru.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 86.00%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alKjzlptPGIf"
      },
      "source": [
        "## BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_crVly9Pc7A"
      },
      "source": [
        "Reference: https://curiousily.com/posts/sentiment-analysis-with-bert-and-hugging-face-using-pytorch-and-python/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvEieNsGPOmF"
      },
      "source": [
        "### BERT Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzoSiCvGPSE6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "193f603fa43b4c299a86bc3e24de3116",
            "d602d562e09d49179d835e1b296c47bf",
            "ab734aae3e3645fca760df978cd99aa5",
            "f682f15f84194cad9740857a609e3d28",
            "b104b0fb9bda4fe58340d40798963586",
            "73fe8f33a1db485d907d55a007fbc170",
            "4c5bc6e60e0547439e9d3b4f680e097f",
            "402da51904b5407e9cf8c224c9a0b9d2"
          ]
        },
        "outputId": "136fee63-d086-46f5-bfcc-74849abaea11"
      },
      "source": [
        "PRE_TRAINED_MODEL_NAME = 'bert-base-cased'\n",
        "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "193f603fa43b4c299a86bc3e24de3116",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=213450.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yp4cuDxu1ViZ"
      },
      "source": [
        "#### Special Token"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwkcGbw9QTcE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a4d58517-b021-4ca7-a162-a968843ff727"
      },
      "source": [
        "# Special Tokens: [SEP] - marker for ending of a sentence\n",
        "tokenizer.sep_token, tokenizer.sep_token_id"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('[SEP]', 102)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRpKkdp1Qmcs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3ca21fba-e0b7-49a6-c2b1-77a16d04c6b0"
      },
      "source": [
        "# Special Tokens: [CLS] - we must add this token to the start of each sentence, so BERT knows we’re doing classification\n",
        "tokenizer.cls_token, tokenizer.cls_token_id"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('[CLS]', 101)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sppb2tBxQ10Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e3331fd7-76fd-4096-d4b8-b2bd878ef1ba"
      },
      "source": [
        "# Special Tokens: padding\n",
        "tokenizer.pad_token, tokenizer.pad_token_id"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('[PAD]', 0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnCBTrvJRT5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "42affb98-dea9-4a64-8333-df471d276a37"
      },
      "source": [
        "# Special Tokens: unkown\n",
        "tokenizer.unk_token, tokenizer.unk_token_id"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('[UNK]', 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waXpAnkr1Zse"
      },
      "source": [
        "#### Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85a4Zli91c0Z"
      },
      "source": [
        "sample_text='DSP is a course for data science and machine learning.'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2i1RYCE1022"
      },
      "source": [
        "tokens = tokenizer.tokenize(sample_text)\n",
        "token_ids = tokenizer.convert_tokens_to_ids(tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUN4fFku11JQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "f00b6efc-110f-4ad3-90d0-d538507fa057"
      },
      "source": [
        "print(f'Sentence: {sample_text}')\n",
        "print(f'Tokens: {tokens}')\n",
        "print(f'Token IDs: {token_ids}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentence: DSP is a course for data science and machine learning.\n",
            "Tokens: ['DS', '##P', 'is', 'a', 'course', 'for', 'data', 'science', 'and', 'machine', 'learning', '.']\n",
            "Token IDs: [18448, 2101, 1110, 170, 1736, 1111, 2233, 2598, 1105, 3395, 3776, 119]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Otxcx0_11Qs"
      },
      "source": [
        "encoding = tokenizer.encode_plus(\n",
        "  sample_text,\n",
        "  max_length=32,\n",
        "  add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n",
        "  return_token_type_ids=False,\n",
        "  padding='max_length',\n",
        "  return_attention_mask=True,\n",
        "  return_tensors='pt',  # Return PyTorch tensors\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlrwyiuP11V4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ef3a9633-6760-46d0-da57-514a87ac5be7"
      },
      "source": [
        "encoding.keys()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['input_ids', 'attention_mask'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXuaRmez11Mr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "f9d44ed0-6408-40c3-990a-449bf7028a02"
      },
      "source": [
        "print(len(encoding['input_ids'][0]))\n",
        "encoding['input_ids'][0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([  101, 18448,  2101,  1110,   170,  1736,  1111,  2233,  2598,  1105,\n",
              "         3395,  3776,   119,   102,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxAV-fMa1zJw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "outputId": "630eeb48-68b7-4623-8748-2b354cf6a597"
      },
      "source": [
        "tokenizer.convert_ids_to_tokens(encoding['input_ids'][0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS]',\n",
              " 'DS',\n",
              " '##P',\n",
              " 'is',\n",
              " 'a',\n",
              " 'course',\n",
              " 'for',\n",
              " 'data',\n",
              " 'science',\n",
              " 'and',\n",
              " 'machine',\n",
              " 'learning',\n",
              " '.',\n",
              " '[SEP]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2AF3Oug3tdK"
      },
      "source": [
        "#### Choosing Sequence Length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_YtHmAI3wr7"
      },
      "source": [
        "token_lens = []\n",
        "for txt in amz_review.sentence:\n",
        "  tokens = tokenizer.encode(txt, max_length=512)\n",
        "  token_lens.append(len(tokens))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUijtAFZ4PDM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "47b6117b-3880-4c12-a229-4db1c2791919"
      },
      "source": [
        "sns.distplot(token_lens)\n",
        "plt.xlim([0, amz_review.sentence.str.len().max()]);\n",
        "plt.xlabel('Token count')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/seaborn/distributions.py:2551: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'Token count')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjgpBuIY6l82"
      },
      "source": [
        "### Train Test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rR5ls9cg621F"
      },
      "source": [
        "df_train, df_test = train_test_split(\n",
        "  amz_review,\n",
        "  test_size=0.2,\n",
        "  stratify=amz_review['target'],\n",
        "  random_state=0\n",
        ")\n",
        "df_val, df_test = train_test_split(\n",
        "  df_test,\n",
        "  test_size=0.5,\n",
        "  stratify=df_test['target'],\n",
        "  random_state=0\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0V3PGcL67NNg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f5a6a250-ff99-41ac-a89f-fe3d1640208f"
      },
      "source": [
        "df_train.shape, df_val.shape, df_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((800, 2), (100, 2), (100, 2))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oaljhy2lAAhD"
      },
      "source": [
        "### Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWQXmkte_jKE"
      },
      "source": [
        "MAX_LEN=20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WULt7obB_nMX"
      },
      "source": [
        "class ReviewDataset(Dataset):\n",
        "  def __init__(self, reviews, targets, tokenizer, max_len):\n",
        "    self.reviews = reviews\n",
        "    self.targets = targets\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "  def __len__(self):\n",
        "    return len(self.reviews)\n",
        "  def __getitem__(self, item):\n",
        "    review = str(self.reviews[item])\n",
        "    target = self.targets[item]\n",
        "    encoding = self.tokenizer.encode_plus(\n",
        "      review,\n",
        "      add_special_tokens=True,\n",
        "      max_length=self.max_len,\n",
        "      return_token_type_ids=False,\n",
        "      pad_to_max_length=True, #depreciated\n",
        "      # padding='max_length',\n",
        "      return_attention_mask=True,\n",
        "      return_tensors='pt',\n",
        "    )\n",
        "    return {\n",
        "      'review_text': review,\n",
        "      'input_ids': encoding['input_ids'].flatten(),\n",
        "      'attention_mask': encoding['attention_mask'].flatten(),\n",
        "      'targets': torch.tensor(target, dtype=torch.long)\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqI3busV8g2z"
      },
      "source": [
        "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
        "  ds = ReviewDataset(\n",
        "    reviews=df['sentence'].to_numpy(),\n",
        "    targets=df['target'].to_numpy(),\n",
        "    tokenizer=tokenizer,\n",
        "    max_len=max_len\n",
        "  )\n",
        "  return DataLoader(\n",
        "    ds,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=8\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YyBUyrFAVEw"
      },
      "source": [
        "BATCH_SIZE = 16\n",
        "train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdAryg5hA_Gm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "e73bdd37-75ae-4622-ff69-8138a7d9cd5d"
      },
      "source": [
        "data = next(iter(train_data_loader))\n",
        "data.keys()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['review_text', 'input_ids', 'attention_mask', 'targets'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kq_simiFBUi7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "fefdd66c-71c6-48c1-cdfe-4aba94edbbe9"
      },
      "source": [
        "print(data['input_ids'].shape)\n",
        "print(data['attention_mask'].shape)\n",
        "print(data['targets'].shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([16, 20])\n",
            "torch.Size([16, 20])\n",
            "torch.Size([16])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9s_p6ixB7QJ"
      },
      "source": [
        "### Build Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTacfkUWBlUQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115,
          "referenced_widgets": [
            "ea857e84ff944d82addf48eba0fd30be",
            "bcd0c19667c341f0ba2f796a1b36733a",
            "964a71cca7a84a76ab01d66d29dd4fdb",
            "255c3f597a3e4d5db352941c683f07a0",
            "e23780be7e814d0fabe988c86ab33acb",
            "896bdba8a78048c18de3768026a1fef2",
            "153b037aa5974882aeecd9bb3a2e007c",
            "fa01f3a5cc854299b734c382f0d9e156",
            "cde02eb9aa5140acbb82ee62a35ebee9",
            "4b1ac4e614484541b19cd6cd78b5489c",
            "4fd08826843d4b00889ef25e089dc462",
            "2c689ff09d0244859d8852893fcaee60",
            "1b5198ad074842ef9e02b60e2a138476",
            "d60c2b288bb84390956b6a4ff47a09e7",
            "883bbe86f4c94c3c8c7c155f53b0f9d8",
            "ca8e32fe69f24306bf253e39f5c086ab"
          ]
        },
        "outputId": "ff2c8ccf-7b98-4725-ca3c-aee7896df9dc"
      },
      "source": [
        "bert_model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ea857e84ff944d82addf48eba0fd30be",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cde02eb9aa5140acbb82ee62a35ebee9",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=435779157.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZiagoVxKCWo1"
      },
      "source": [
        "last_hidden_state, pooled_output = bert_model(\n",
        "  input_ids=encoding['input_ids'],\n",
        "  attention_mask=encoding['attention_mask']\n",
        ")\n",
        "# The last_hidden_state is a sequence of hidden states of the last layer of the model.\n",
        "# Obtaining the pooled_output is done by applying the BertPooler on last_hidden_state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0W_187OrCwFv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e90f8533-d37f-4661-b657-574c0f549c9f"
      },
      "source": [
        "last_hidden_state.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 32, 768])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzJVFRr-EIad"
      },
      "source": [
        "32 is the max_length for the example text. There is one hidden state for each of the 32 tokens. 768 is the number of hidden units in base BERT."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VN4GJrG0CwMb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "251c67bb-05d5-421a-e554-2ec7b6cdd16f"
      },
      "source": [
        "bert_model.config.hidden_size"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "768"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lF5Am9xKCv49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2df18292-ffa3-45f0-de3b-065d00f5c591"
      },
      "source": [
        "pooled_output.shape # Embedding output. We can think of it as a summary of the content."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 768])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzWPui-ZFFlX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bc8f16b3-3113-4b03-ff39-5f08cdb89735"
      },
      "source": [
        "pooled_output"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.6969,  0.4655,  0.9997, -0.9845,  0.9469,  0.8062,  0.9525, -0.9912,\n",
              "         -0.9315, -0.6231,  0.9614,  0.9953, -0.9980, -0.9996,  0.7337, -0.9349,\n",
              "          0.9794, -0.6189, -0.9999, -0.6448, -0.3649, -0.9996,  0.1486,  0.9472,\n",
              "          0.9441,  0.0524,  0.9681,  0.9999,  0.8278, -0.3314,  0.1614, -0.9769,\n",
              "          0.8667, -0.9980,  0.1179,  0.2168,  0.7371, -0.2879,  0.8317, -0.9285,\n",
              "         -0.6839, -0.7559,  0.5917, -0.5311,  0.8996,  0.2129,  0.2172, -0.1329,\n",
              "         -0.1384,  0.9998, -0.9070,  0.9941, -0.9935,  0.9882,  0.9785,  0.4273,\n",
              "          0.9866,  0.1613, -0.9988,  0.3731,  0.9487,  0.1924,  0.8418, -0.1986,\n",
              "          0.0750, -0.4077, -0.7958,  0.1057, -0.4370,  0.2784,  0.2699,  0.3081,\n",
              "          0.9592, -0.8326,  0.0282, -0.8911,  0.1940, -0.9996,  0.9486,  0.9999,\n",
              "          0.6675, -0.9991,  0.9847, -0.3002, -0.5742,  0.5608, -0.9989, -0.9987,\n",
              "          0.0415, -0.6804,  0.8382, -0.9746,  0.4034, -0.8318,  0.9999, -0.9368,\n",
              "         -0.1425,  0.3102,  0.8687, -0.5025, -0.7619,  0.8606,  0.9991, -0.9969,\n",
              "          0.9983,  0.3134, -0.9290, -0.7096,  0.5966,  0.1965,  0.9788, -0.9810,\n",
              "         -0.6340,  0.0436,  0.8502, -0.8418,  0.9705,  0.7962, -0.2665,  0.9999,\n",
              "         -0.0208,  0.9533,  0.9948,  0.9092, -0.8054, -0.2157, -0.5536,  0.8321,\n",
              "         -0.4203, -0.4590,  0.7380, -0.9813, -0.9980,  0.9983, -0.2413,  0.9999,\n",
              "         -0.9977,  0.9919, -0.9999, -0.7627, -0.7961, -0.1036, -0.9770,  0.1567,\n",
              "          0.9699, -0.1019, -0.8589, -0.7127,  0.4701, -0.6896,  0.6233,  0.6402,\n",
              "         -0.9162,  0.9895,  0.9965,  0.9316,  0.9662,  0.2874, -0.9353,  0.8883,\n",
              "          0.9475, -0.9988,  0.7751, -0.9941,  0.9984,  0.9496,  0.6759, -0.9960,\n",
              "          0.9997, -0.3160, -0.0590,  0.0381, -0.1753, -0.9991,  0.3722,  0.4113,\n",
              "          0.7739,  0.9993, -0.9851,  0.9996,  0.9047,  0.0970,  0.7173,  0.9986,\n",
              "         -0.9893, -0.9353, -0.9765,  0.3605,  0.6544,  0.6718,  0.2546,  0.9384,\n",
              "          0.9991,  0.5469, -0.9976, -0.3620,  0.9308, -0.1451,  0.9999,  0.0978,\n",
              "         -0.9995, -0.7514,  0.9076,  0.8067, -0.3404,  0.9094, -0.6723, -0.1253,\n",
              "          0.9803, -0.9875,  0.9988, -0.0418,  0.6299,  0.8387,  0.9709, -0.5859,\n",
              "         -0.1484,  0.1487, -0.6645,  0.9997, -0.9989, -0.1408,  0.2901, -0.9884,\n",
              "         -0.9940,  0.9105, -0.0336, -0.7218, -0.1761,  0.3999,  0.1698,  0.8678,\n",
              "          0.9789, -0.5468, -0.3833, -0.9996, -0.9980, -0.8170, -0.9382,  0.0392,\n",
              "          0.6405, -0.4003, -0.8921, -0.9985,  0.9223,  0.8218, -0.8518, -0.2434,\n",
              "         -0.5405, -0.9986,  0.5582, -0.7620, -0.9975,  0.9986, -0.7797,  0.9946,\n",
              "          0.8837, -0.9843,  0.7636, -0.9989, -0.1712, -0.9926,  0.2713,  0.3625,\n",
              "         -0.7090,  0.1106,  0.9796, -0.9515, -0.7412,  0.6515, -0.9997,  0.8709,\n",
              "         -0.1233,  0.9974,  0.8833, -0.1863,  0.9559,  0.8632, -0.9786, -0.9994,\n",
              "          0.8840,  0.9574, -0.9807, -0.1371,  0.9998, -0.9991, -0.8377, -0.9406,\n",
              "         -0.9751, -0.9991,  0.3271, -0.7938,  0.2600,  0.9627,  0.3236,  0.1820,\n",
              "          0.9733,  0.9743,  0.2367, -0.0482, -0.0022, -0.9725, -0.9803,  0.7270,\n",
              "          0.1253, -0.9999,  0.9995, -0.9660,  0.9914,  0.8928, -0.9965,  0.8138,\n",
              "          0.2042, -0.9495, -0.0537,  0.9998,  0.9523, -0.0949,  0.1552,  0.8779,\n",
              "         -0.1633,  0.5379, -0.7840, -0.4109,  0.1560, -0.8905,  0.9797,  0.6206,\n",
              "         -0.9629,  0.9956,  0.1320,  0.7233, -0.6549,  0.8791,  0.9763,  0.0067,\n",
              "         -0.1722, -0.1261, -0.6356, -0.9759,  0.1473, -0.9967, -0.0455,  0.9355,\n",
              "          0.9430, -0.9080,  0.9770, -0.1142,  0.8280, -0.9985,  0.9999, -0.9845,\n",
              "          0.1913,  0.5906, -0.9237, -0.2967,  0.9822,  0.9853,  0.9662, -0.8641,\n",
              "         -0.4676,  0.7618,  0.8904, -0.9802, -0.0224, -0.9994, -0.5468,  0.9955,\n",
              "          0.9983, -0.0285, -0.3838, -0.9977,  0.9225, -0.8950, -0.7778, -0.0992,\n",
              "         -0.7235,  0.6580,  0.9985, -0.5245,  0.7296,  0.1786, -0.9571,  0.8326,\n",
              "          0.7173,  0.9995, -0.9391,  0.2374,  0.9674, -0.1605, -0.6367,  0.4816,\n",
              "          0.9989, -0.7594, -0.2219, -0.9991, -0.0522, -0.5478, -0.2315, -0.4794,\n",
              "         -0.0067, -0.7740,  0.9400,  0.0915,  0.8002, -0.1456,  0.9686, -0.0752,\n",
              "         -0.0426, -0.2477, -0.1640,  0.4746,  0.1574,  0.9593, -0.9247,  0.9996,\n",
              "         -0.4804, -0.9999, -0.9976, -0.6102, -0.9995,  0.5808, -0.9788,  0.9597,\n",
              "          0.8962, -0.9987, -0.9994, -0.9907, -0.9878,  0.7643,  0.6001, -0.0170,\n",
              "          0.1547,  0.6501,  0.0200,  0.2532, -0.0429, -0.9055, -0.2064, -0.9989,\n",
              "          0.7801, -0.9999, -0.6456,  0.9971, -0.9949, -0.9131, -0.8831, -0.9040,\n",
              "         -0.7094,  0.4176,  0.9649, -0.2901, -0.6076, -0.9995,  0.9676, -0.8386,\n",
              "          0.0683, -0.7218, -0.9512,  0.9992,  0.8652, -0.1809, -0.0505, -0.9980,\n",
              "          0.9795, -0.9215, -0.8183, -0.9557,  0.0815, -0.9086, -0.9996,  0.1527,\n",
              "          0.9973,  0.9830,  0.9742,  0.4179, -0.3739, -0.9392,  0.0550, -0.9999,\n",
              "          0.7244,  0.7669, -0.9483, -0.7062,  0.9609,  0.9472, -0.8768, -0.9575,\n",
              "          0.8238,  0.4537,  0.9605, -0.3825, -0.2670,  0.3803, -0.0199, -0.9798,\n",
              "         -0.9360,  0.9875, -0.9991,  0.9448,  0.9946,  0.9987,  0.0106, -0.0541,\n",
              "         -0.9766, -0.9872, -0.6257,  0.3684, -0.9998,  0.9998, -0.9999,  0.5182,\n",
              "         -0.3960,  0.8474,  0.9803, -0.2589, -0.9998, -0.9996,  0.8748,  0.0856,\n",
              "          0.9780,  0.2758,  0.1419, -0.5783, -0.0039,  0.9873, -0.8636, -0.2501,\n",
              "         -0.9988,  0.9993,  0.5284, -0.9944,  0.9894, -0.9989,  0.8470,  0.9361,\n",
              "          0.8880,  0.9656, -0.9991,  0.9999, -0.9997,  0.9916, -0.9999, -0.9994,\n",
              "          0.9995, -0.9818, -0.7478, -0.9990, -0.9969,  0.6434,  0.0680, -0.5600,\n",
              "          0.9721, -0.9996, -0.9951, -0.3564, -0.9207, -0.6693,  0.9958, -0.5726,\n",
              "          0.9470, -0.1900,  0.9364,  0.2848,  0.9944,  0.9891, -0.6861, -0.6344,\n",
              "         -0.9832,  0.9887, -0.5411,  0.3346,  0.9535,  0.2278, -0.5842,  0.5116,\n",
              "         -0.9937,  0.5621, -0.4517,  0.9444,  0.8937,  0.7977,  0.0388, -0.5262,\n",
              "         -0.1885, -0.9845,  0.4822, -0.9988,  0.9806, -0.9324, -0.0466, -0.4458,\n",
              "          0.3364, -0.9176,  0.9988,  0.9969, -0.9938,  0.0329,  0.9678, -0.7220,\n",
              "          0.9428, -0.9801,  0.0708,  0.9189, -0.7052,  0.9679, -0.0090, -0.1470,\n",
              "          0.9667, -0.9868, -0.8782, -0.7092,  0.3057,  0.2824, -0.9355,  0.0375,\n",
              "          0.9836, -0.3185, -0.9992,  0.9567, -0.9980, -0.1331,  0.9490, -0.6289,\n",
              "          0.9997, -0.7225,  0.1388,  0.1854, -0.9993, -0.9989, -0.0206, -0.1744,\n",
              "         -0.9420,  0.9994, -0.1565,  0.6936, -0.9998,  0.3195,  0.9951,  0.2433,\n",
              "          0.8703, -0.6784, -0.9609, -0.9085, -0.7463, -0.1091,  0.8635, -0.7794,\n",
              "         -0.8988, -0.7162,  0.9999, -0.9935, -0.9294, -0.9854,  0.5838,  0.8384,\n",
              "          0.4404,  0.2095, -0.7947,  0.9114, -0.8539,  0.9926, -0.9855, -0.9830,\n",
              "          0.9995,  0.5829, -0.9932,  0.1716, -0.3380,  0.2104,  0.0339,  0.6885,\n",
              "         -0.6029, -0.1853, -0.9877,  0.8711, -0.8027, -0.9610, -0.6469, -0.3475,\n",
              "         -0.9908,  0.9850,  0.9336,  0.9999, -0.9995,  0.8015,  0.0860,  0.9974,\n",
              "          0.1247, -0.6129,  0.8836,  0.9990, -0.6280,  0.8484, -0.0808,  0.0123,\n",
              "          0.2978, -0.3118,  0.9971, -0.9451, -0.0489, -0.9451, -0.9998,  0.9998,\n",
              "          0.0038,  0.9821,  0.2535,  0.7701, -0.8272,  0.9483, -0.9746, -0.9184,\n",
              "         -0.9999,  0.2258, -0.9937, -0.9686, -0.0216,  0.8771, -0.9985, -0.9610,\n",
              "         -0.4172, -0.9999,  0.9011, -0.9864, -0.8250, -0.9536,  0.9987, -0.3165,\n",
              "         -0.5076,  0.9330, -0.9303,  0.9029,  0.9585,  0.3391,  0.3007, -0.0236,\n",
              "         -0.7115, -0.9937, -0.9199, -0.9627,  0.8638, -0.9552, -0.7298,  0.9929,\n",
              "          0.9615, -0.9985, -0.9909,  0.9968,  0.1128,  0.9733, -0.4586, -0.9996,\n",
              "         -0.9997,  0.1043, -0.1337,  0.9833, -0.3389,  0.9848,  0.8156, -0.3953,\n",
              "          0.4138, -0.5041, -0.1836, -0.0505, -0.1258,  0.9999, -0.6989,  0.9711]],\n",
              "       grad_fn=<TanhBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2ztZbdVFj0X"
      },
      "source": [
        "class SentimentClassifier(nn.Module):\n",
        "\n",
        "  def __init__(self, n_classes):\n",
        "    super(SentimentClassifier, self).__init__()\n",
        "    self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
        "    self.drop = nn.Dropout(p=0.1)\n",
        "    self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
        "  \n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    _, pooled_output = self.bert(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask\n",
        "    )\n",
        "    output = self.drop(pooled_output)\n",
        "    return self.out(output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zQnrQcLGmMs"
      },
      "source": [
        "class_names = [0,1]\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = SentimentClassifier(len(class_names))\n",
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kB64me3MHYX_"
      },
      "source": [
        "input_ids = data['input_ids'].to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGpxkm2bHYmd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "outputId": "a90a0f58-439a-4305-c053-1dbff8c650c4"
      },
      "source": [
        "input_ids # first batch of the train data loader"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  101,  1398,  1107,  1155,   117,   146,   112,   182,  2385,  8723,\n",
              "          1114,  1142,  4779,   119,   102,     0,     0,     0,     0,     0],\n",
              "        [  101,   146,  1138,  1106,  1329,  1103, 10471,  3811,  9641,  1116,\n",
              "          2136,   117,  1133,  1122, 12543,  1113,  2785,  1218,   119,   102],\n",
              "        [  101,  1188,  3317,  1110,  7891,  1111,  1234,  1176,  1143,  2133,\n",
              "          4942,  1132,  1304,  7246,   119,   102,     0,     0,     0,     0],\n",
              "        [  101,  5853,  1112,  1758,   119,   102,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
              "        [  101,   146,  1108,  1682,  1106,  1202,  1490, 17693,  1158,  1107,\n",
              "          1103,  1610,  1114,  1185,  2463,   119,   102,     0,     0,     0],\n",
              "        [  101,  1422, 11864,   111,   146,  1208,  1243,  1632,  7602,   119,\n",
              "           138,  1376,  5865,   117,  1133,  2099,  1110,  1632,   119,   102],\n",
              "        [  101,  2098,   170,  1703,  7305,   119,   102,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
              "        [  101,  1109,  3251,  1674,  1243,   188, 13601, 17363,  3253,  1272,\n",
              "          1122, 13193,  1240,  3811,  1105,  1339,   119,   102,     0,     0],\n",
              "        [  101,  1109,  2179,  3370,   142, 23206, 16941, 14424,  2162,  3663,\n",
              "           145, 14697,   106,   102,     0,     0,     0,     0,     0,     0],\n",
              "        [  101,   146,  1125,   159,  9866,  9515,   123,  1201,  2403,  1105,\n",
              "          1541,  3851,  1147,  1555,   119,   102,     0,     0,     0,     0],\n",
              "        [  101,  1109,  4504,   117,  1780,  6317,  1120,  1126,  7757,   122,\n",
              "           119,   124,  1143,  2571,  8508, 16056,  3447,   117, 19566,   102],\n",
              "        [  101,  3982,  1136,  2816,   119,   102,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
              "        [  101,  1753,  1178,  1209,  1122, 13734,  1240,  1591,   117,  1133,\n",
              "          1336,  1145,  9046,   175,  1616,  1122,   119,   102,     0,     0],\n",
              "        [  101,   146,  2010,   112,   189, 18029,  9241,  1142,  3317,   119,\n",
              "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
              "        [  101,  1188,  1110,   170,   159,  9637,  3663,  1903,  2179,  1114,\n",
              "          2213,  7105,  1297,  1115,  5049,  1113,   170,  4780,  2443,   102],\n",
              "        [  101,   146,  1567,  1142,  6095,   118,  1122,  3643,  1143,  1106,\n",
              "          7543,  1251,  8715,   118, 20453,  4442,  1106,  1139,  7054,   102]],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-fyfmDIHYux"
      },
      "source": [
        "attention_mask = data['attention_mask'].to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gw3mUQkeHY4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "a12d53b8-4615-471e-cab9-4d81c33e61e6"
      },
      "source": [
        "attention_mask"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
              "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdwVk-EHHYz6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "18aca991-164b-4f2b-c63e-ef4563fd80ba"
      },
      "source": [
        "print(input_ids.shape) # batch size x seq length\n",
        "print(attention_mask.shape) # batch size x seq length"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([16, 20])\n",
            "torch.Size([16, 20])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frVKM_X_HYrC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "6d39c595-b7b7-4b11-93e2-38a4dae3129f"
      },
      "source": [
        "from torch.nn import functional as F\n",
        "F.softmax(model(input_ids, attention_mask), dim=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.7172, 0.2828],\n",
              "        [0.7152, 0.2848],\n",
              "        [0.7219, 0.2781],\n",
              "        [0.5825, 0.4175],\n",
              "        [0.6124, 0.3876],\n",
              "        [0.6941, 0.3059],\n",
              "        [0.6391, 0.3609],\n",
              "        [0.7603, 0.2397],\n",
              "        [0.6431, 0.3569],\n",
              "        [0.6887, 0.3113],\n",
              "        [0.6396, 0.3604],\n",
              "        [0.7245, 0.2755],\n",
              "        [0.5858, 0.4142],\n",
              "        [0.7517, 0.2483],\n",
              "        [0.6700, 0.3300],\n",
              "        [0.7359, 0.2641]], device='cuda:0', grad_fn=<SoftmaxBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZILtX2sGJlUd"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCdAZk-PHYdJ"
      },
      "source": [
        "EPOCHS = 10\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
        "total_steps = len(train_data_loader) * EPOCHS\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "  optimizer,\n",
        "  num_warmup_steps=0,\n",
        "  num_training_steps=total_steps\n",
        ")\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2ZFX8rkHYTc"
      },
      "source": [
        "def train_epoch(\n",
        "  model,\n",
        "  data_loader,\n",
        "  loss_fn,\n",
        "  optimizer,\n",
        "  device,\n",
        "  scheduler,\n",
        "  n_examples\n",
        "):\n",
        "  model = model.train()\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "  for d in data_loader:\n",
        "    input_ids = d[\"input_ids\"].to(device)\n",
        "    attention_mask = d[\"attention_mask\"].to(device)\n",
        "    targets = d[\"targets\"].to(device)\n",
        "    outputs = model(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask\n",
        "    )\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    loss = loss_fn(outputs, targets)\n",
        "    correct_predictions += torch.sum(preds == targets)\n",
        "    losses.append(loss.item())\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJ8_Z66BK9Cl"
      },
      "source": [
        "The scheduler gets called every time a batch is fed to the model. We’re avoiding exploding gradients by clipping the gradients of the model using clipgrad_norm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_TambFVLKey"
      },
      "source": [
        "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
        "  model = model.eval()\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      targets = d[\"targets\"].to(device)\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask\n",
        "      )\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "      loss = loss_fn(outputs, targets)\n",
        "      correct_predictions += torch.sum(preds == targets)\n",
        "      losses.append(loss.item())\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtUhDMBvLZk4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "de4c4151-432e-4f1e-f0d5-1944ccf4d7f8"
      },
      "source": [
        "%%time\n",
        "history = defaultdict(list)\n",
        "best_accuracy = 0\n",
        "for epoch in range(EPOCHS):\n",
        "  print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
        "  print('-' * 10)\n",
        "  train_acc, train_loss = train_epoch(\n",
        "    model,\n",
        "    train_data_loader,\n",
        "    loss_fn,\n",
        "    optimizer,\n",
        "    device,\n",
        "    scheduler,\n",
        "    len(df_train)\n",
        "  )\n",
        "  print(f'Train loss {train_loss} accuracy {train_acc}')\n",
        "  val_acc, val_loss = eval_model(\n",
        "    model,\n",
        "    val_data_loader,\n",
        "    loss_fn,\n",
        "    device,\n",
        "    len(df_val)\n",
        "  )\n",
        "  print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
        "  print()\n",
        "  history['train_acc'].append(train_acc)\n",
        "  history['train_loss'].append(train_loss)\n",
        "  history['val_acc'].append(val_acc)\n",
        "  history['val_loss'].append(val_loss)\n",
        "  if val_acc > best_accuracy:\n",
        "    torch.save(model.state_dict(), 'best_model_state.bin')\n",
        "    best_accuracy = val_acc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.48665341928601263 accuracy 0.77875\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.40361499999250683 accuracy 0.86\n",
            "\n",
            "Epoch 2/10\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.3057754861935973 accuracy 0.89375\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.5838208831846714 accuracy 0.84\n",
            "\n",
            "Epoch 3/10\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.23230868175625802 accuracy 0.93375\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.4291017422718661 accuracy 0.88\n",
            "\n",
            "Epoch 4/10\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.1372149334475398 accuracy 0.96125\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.5677490872996194 accuracy 0.9\n",
            "\n",
            "Epoch 5/10\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.09560541208833456 accuracy 0.9775\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.550334817197706 accuracy 0.91\n",
            "\n",
            "Epoch 6/10\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.05945142090320587 accuracy 0.9862500000000001\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.5562760450744203 accuracy 0.9\n",
            "\n",
            "Epoch 7/10\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.03840944638475776 accuracy 0.98875\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.5318383054940828 accuracy 0.92\n",
            "\n",
            "Epoch 8/10\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.014008238068781794 accuracy 0.99375\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.5971973272971809 accuracy 0.91\n",
            "\n",
            "Epoch 9/10\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.008814937287243084 accuracy 0.995\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.6093123152220089 accuracy 0.92\n",
            "\n",
            "Epoch 10/10\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.0073237852938473225 accuracy 0.99625\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.6121098469344101 accuracy 0.93\n",
            "\n",
            "CPU times: user 44.7 s, sys: 17.1 s, total: 1min 1s\n",
            "Wall time: 1min 20s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxYknxMxQxc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "f6ba303f-2cf9-4cdc-c2f5-8b1b9d362fc6"
      },
      "source": [
        "plt.plot(history['train_acc'], label='train accuracy')\n",
        "plt.plot(history['val_acc'], label='validation accuracy')\n",
        "plt.title('Training history')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.ylim([0, 1]);"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLlgG81qVbES",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "ae551197-9d68-46d8-f3b5-b06053a4cee6"
      },
      "source": [
        "test_acc, _ = eval_model(\n",
        "  model,\n",
        "  test_data_loader,\n",
        "  loss_fn,\n",
        "  device,\n",
        "  len(df_test)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IG1kWi-NVjcF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8125cbdc-ad84-4272-fc9f-0b7802a69cb0"
      },
      "source": [
        "test_acc.item()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.88"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMlP1kZYB-0z"
      },
      "source": [
        "### Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjLNAMzAV3J1"
      },
      "source": [
        "def get_predictions(model, data_loader):\n",
        "  model = model.eval()\n",
        "  review_texts = []\n",
        "  predictions = []\n",
        "  prediction_probs = []\n",
        "  real_values = []\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "      texts = d[\"review_text\"]\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      targets = d[\"targets\"].to(device)\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask\n",
        "      )\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "      review_texts.extend(texts)\n",
        "      predictions.extend(preds)\n",
        "      prediction_probs.extend(outputs)\n",
        "      real_values.extend(targets)\n",
        "  predictions = torch.stack(predictions).cpu()\n",
        "  prediction_probs = torch.stack(prediction_probs).cpu()\n",
        "  real_values = torch.stack(real_values).cpu()\n",
        "  return review_texts, predictions, prediction_probs, real_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6C7JeAWWM0G",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "99368166-ac2f-4d4c-edd0-af92dfc6a2ac"
      },
      "source": [
        "y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(\n",
        "  model,\n",
        "  test_data_loader\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iQmnweiWTrj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c4db0785-cc0b-421d-de20-282ac4096ddb"
      },
      "source": [
        "y_review_texts"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Price is good too.',\n",
              " 'Small, sleek, impressive looking, practical setup with ample storage in place.',\n",
              " \"I'm returning them.\",\n",
              " 'Jawbone Era is awesome too!',\n",
              " 'As an earlier review noted, plug in this charger and nothing happens.',\n",
              " 'How can that be?The audio quality is poor.',\n",
              " 'Att is not clear, sound is very distorted and you have to yell when you talk.',\n",
              " 'I had ordered a motorola data cable, got a very well finished and working product.',\n",
              " 'The bottowm line...another worthless, cheap gimmick from Sprint.',\n",
              " 'Everything worked on the first try.The device was certainly engineered in a clever way and the construction feels good.',\n",
              " 'The internet access was fine, it the rare instance that it worked.',\n",
              " \"I've had this bluetoooth headset for some time now and still not comfortable with the way it fits on the ear.\",\n",
              " 'Great phone!.',\n",
              " 'Beautiful styling though.',\n",
              " 'Battery life is also great!',\n",
              " 'Really good product.',\n",
              " \"No ear loop needed, it's tiny and the sound is great.\",\n",
              " 'An Awesome New Look For Fall 2000!.',\n",
              " 'This is a great deal.',\n",
              " \"I'm pleased.\",\n",
              " 'And the sound quality is great.',\n",
              " 'Simple, lightweight and great fit.',\n",
              " 'The construction of the headsets is poor.',\n",
              " 'It is easy to turn on and off when you are in the car and the volume controls are quite accessable.',\n",
              " 'I bought two of them and neither will charge.',\n",
              " 'Perhaps my phone is defective, but people cannot hear me when I use this.',\n",
              " 'I purcashed this for the car charger and it does not work.',\n",
              " 'Problem is that the ear loops are made of weak material and break easily.',\n",
              " 'I am going to have to be the first to negatively review this product.',\n",
              " 'WORST PHONE EVER.',\n",
              " 'Good value, works fine - power via USB, car, or wall outlet.',\n",
              " \"My phone sounded OK ( not great - OK), but my wife's phone was almost totally unintelligible, she couldn't understand a word being said on it.\",\n",
              " 'Arrived quickly and much less expensive than others being sold.',\n",
              " 'When I placed my treo into the case, not only was it NOT snug, but there was A LOT of extra room on the sides.',\n",
              " 'It looses connection to the phone even when it is within a couple feet between the two.',\n",
              " 'i would advise to not purchase this item it never worked very well.',\n",
              " 'My experience was terrible..... This was my fourth bluetooth headset, and while it was much more comfortable than my last Jabra (which I HATED!!!',\n",
              " 'Does not charge the Cingular (ATT) 8525 phone.',\n",
              " \"however, my girl was complain that some time the phone doesn't wake up like normal phone does.\",\n",
              " 'Sprint charges for this service.',\n",
              " 'Appears to actually outperform the original battery from China that came with my V325i.',\n",
              " 'Iam very pleased with my purchase.',\n",
              " 'After receiving and using the product for just 2 days it broke.',\n",
              " 'Awkward to use and unreliable.',\n",
              " 'Worst Customer Service Ever.',\n",
              " 'I Was Hoping for More.',\n",
              " '2 thumbs up to this seller',\n",
              " 'I kept catching the cable on the seat and I had to pull the phone out to turn it on an off.',\n",
              " 'I have used several phone in two years, but this one is the best.',\n",
              " 'The only thing that disappoint me is the infra red port (irda).',\n",
              " \"im surprised this is a good quality car charger and there's not much reviews about it.\",\n",
              " 'It felt too light and \"tinny.\".',\n",
              " 'I have tried these cables with my computer and my iPod and it works just fine.',\n",
              " 'This is a good battery, and it got here really fast.',\n",
              " 'camera color balance is AWFUL.',\n",
              " 'Good case!.',\n",
              " 'Much less than the jawbone I was going to replace it with.',\n",
              " 'Not a good bargain.',\n",
              " \"Better than you'd expect.\",\n",
              " \"You'll love how thin it is.\",\n",
              " 'Poor Reliability.',\n",
              " 'It makes very strange ticking noises before it ends the call.',\n",
              " 'VERY DISAPPOINTED.',\n",
              " 'We have tried 2 units and they both failed within 2 months.. Pros',\n",
              " \"don't waste your money.\",\n",
              " 'Overall, I would recommend this phone over the new Walkman.',\n",
              " 'I have this phone and it is a thorn in my side, I really abhor it.',\n",
              " 'I had to purchase a different case.',\n",
              " 'Excellent product, I am very satisfied with the purchase.',\n",
              " \"I'll be looking for a new earpiece.\",\n",
              " 'I highly recommend these and encourage people to give them a try.',\n",
              " 'Works fine.',\n",
              " 'Those phones are working just fine now.',\n",
              " 'This is a great product..... sure beats using your fingers!.',\n",
              " 'Sprint - terrible customer service.',\n",
              " \"I connected my wife's bluetooth,(Motorola HS850) to my phone and it worked like a charm whether the phone was in my pocket or the case.\",\n",
              " 'Better than expected.',\n",
              " 'This is so embarassing and also my ears hurt if I try to push the ear plug into my ear.',\n",
              " \"Couldn't figure it out\",\n",
              " 'But despite these few flaws, this case is of exceptional quality and well worth the additional costs of owning an official OEM product.',\n",
              " 'The case is a flimsy piece of plastic and has no front or side protection whatsoever.',\n",
              " 'The battery is working well as a replacement for the original that came with the phone over 2 years ago.',\n",
              " 'I have had mine for about a year and this Christmas I bought some for the rest of the family.',\n",
              " 'It feels more comfortable than most headsets because I wear glasses and that gets in the way sometimes.',\n",
              " \"I'll be drivng along, and my headset starts ringing for no reason.\",\n",
              " 'This is infuriating.',\n",
              " 'I could not recommend these more.',\n",
              " 'I would recommend this.',\n",
              " 'Excellent product for the price.',\n",
              " 'Items stated as included from the description ARE NOT INCLUDED.',\n",
              " 'Love this product.',\n",
              " 'Car charger as well as AC charger are included to make sure you never run out of juice.Highy recommended',\n",
              " \"Unreliable - I'm giving up.\",\n",
              " 'I love the ringtones because they are so upbeat!',\n",
              " 'The iGo chargers and tips are really great.',\n",
              " 'but it is great, i would really recommend it',\n",
              " 'Nice headphones for the price and they work great!',\n",
              " 'Same problem as others have mentioned.',\n",
              " 'However, the keypads are so tinny that I sometimes reach the wrong buttons.',\n",
              " 'The volume for the ringer is REAL good (you have choices how loud).']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Yeg9QKbWZeI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "05f191e1-c6c9-4925-bf42-1b2c7b3f5605"
      },
      "source": [
        "y_pred"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,\n",
              "        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
              "        1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,\n",
              "        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n",
              "        1, 0, 0, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0B9SfPU5WcWO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "97e7e0bd-9963-4d56-b6ae-4492f5b60253"
      },
      "source": [
        "y_pred_probs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-3.4317,  4.2389],\n",
              "        [-3.4933,  4.1034],\n",
              "        [-0.6074,  1.3725],\n",
              "        [-2.6274,  4.0833],\n",
              "        [ 3.4333, -4.2102],\n",
              "        [ 3.6744, -4.5175],\n",
              "        [ 3.5783, -4.4066],\n",
              "        [-3.1909,  4.3594],\n",
              "        [ 3.6598, -4.4653],\n",
              "        [-3.4637,  4.2456],\n",
              "        [-3.3205,  4.3288],\n",
              "        [ 2.8478, -3.2403],\n",
              "        [-3.4992,  4.0351],\n",
              "        [-3.4609,  4.1983],\n",
              "        [-3.3334,  4.3588],\n",
              "        [-3.5069,  4.1292],\n",
              "        [-3.0566,  4.3684],\n",
              "        [ 2.2552, -3.1006],\n",
              "        [-3.4409,  4.2362],\n",
              "        [-3.4370,  4.0501],\n",
              "        [-3.3564,  4.3334],\n",
              "        [-3.5769,  3.9179],\n",
              "        [ 3.6613, -4.4544],\n",
              "        [-1.6006,  2.9504],\n",
              "        [ 1.9262, -2.6830],\n",
              "        [ 3.6980, -4.4580],\n",
              "        [ 3.5654, -4.3720],\n",
              "        [ 3.7235, -4.5513],\n",
              "        [ 3.5852, -4.4214],\n",
              "        [-2.9117,  4.0679],\n",
              "        [-2.7459,  4.2377],\n",
              "        [ 1.0481, -1.3960],\n",
              "        [ 2.5555, -3.6527],\n",
              "        [ 3.0256, -3.8936],\n",
              "        [ 3.5741, -4.3791],\n",
              "        [ 3.6051, -4.3118],\n",
              "        [ 3.6818, -4.5054],\n",
              "        [ 3.6296, -4.4579],\n",
              "        [ 3.5956, -4.3025],\n",
              "        [ 3.0260, -3.9900],\n",
              "        [ 2.5073, -3.3443],\n",
              "        [-3.4720,  4.1121],\n",
              "        [ 3.7407, -4.5260],\n",
              "        [ 3.7132, -4.4703],\n",
              "        [ 3.7497, -4.5083],\n",
              "        [ 1.5714, -2.5251],\n",
              "        [-0.4785,  0.7685],\n",
              "        [ 0.0933, -1.1102],\n",
              "        [-3.3001,  4.3111],\n",
              "        [ 3.6127, -4.3975],\n",
              "        [ 2.8450, -3.8216],\n",
              "        [ 3.0180, -4.2145],\n",
              "        [-3.2170,  4.3717],\n",
              "        [-3.5089,  4.2129],\n",
              "        [-1.8252,  3.2551],\n",
              "        [-3.5131,  3.9965],\n",
              "        [ 3.2156, -4.0194],\n",
              "        [ 3.5575, -4.3470],\n",
              "        [-3.1531,  4.1897],\n",
              "        [-0.0468,  0.8220],\n",
              "        [ 3.7375, -4.5187],\n",
              "        [ 3.1511, -4.1153],\n",
              "        [ 0.5266, -1.0225],\n",
              "        [ 3.6275, -4.4453],\n",
              "        [ 3.7083, -4.5047],\n",
              "        [-3.3896,  4.2095],\n",
              "        [ 3.2046, -4.1334],\n",
              "        [-2.8528,  4.0679],\n",
              "        [-3.4793,  4.1511],\n",
              "        [-0.9337,  1.8245],\n",
              "        [-3.3616,  4.2955],\n",
              "        [-3.5060,  4.0820],\n",
              "        [-3.4834,  4.1921],\n",
              "        [-3.1410,  4.4034],\n",
              "        [ 3.7484, -4.5372],\n",
              "        [-1.1978,  2.1743],\n",
              "        [-3.2261,  4.1240],\n",
              "        [ 3.1847, -4.1382],\n",
              "        [ 3.5777, -4.2172],\n",
              "        [-2.7395,  4.2070],\n",
              "        [ 3.0214, -3.9821],\n",
              "        [-2.1517,  3.6592],\n",
              "        [-2.0417,  3.6254],\n",
              "        [-2.9496,  4.2965],\n",
              "        [ 3.0103, -3.9312],\n",
              "        [-2.1810,  2.7610],\n",
              "        [ 3.5419, -4.2058],\n",
              "        [-3.4838,  4.0377],\n",
              "        [-3.3253,  4.3052],\n",
              "        [ 2.9350, -3.5574],\n",
              "        [-3.5045,  4.0466],\n",
              "        [-0.3000,  1.0253],\n",
              "        [ 3.3716, -4.3404],\n",
              "        [-3.4754,  4.1935],\n",
              "        [-2.7206,  4.1948],\n",
              "        [-3.4660,  4.2730],\n",
              "        [-3.3097,  4.3644],\n",
              "        [ 3.3463, -4.1592],\n",
              "        [ 3.5752, -4.3973],\n",
              "        [-3.0945,  4.4098]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlqqMVDBWmy_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "2f7b9db7-2320-4dad-e8d9-fb6697e5ea57"
      },
      "source": [
        "y_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
              "        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,\n",
              "        1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,\n",
              "        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n",
              "        1, 0, 0, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dBXLvHTXYSB"
      },
      "source": [
        "### Performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aisYil1vW1Bp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "09fd9616-0acd-4000-8554-27d5da73ca1e"
      },
      "source": [
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.86      0.88        50\n",
            "           1       0.87      0.90      0.88        50\n",
            "\n",
            "    accuracy                           0.88       100\n",
            "   macro avg       0.88      0.88      0.88       100\n",
            "weighted avg       0.88      0.88      0.88       100\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4G3_yPlXasB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "outputId": "affa17f6-b430-40de-97e7-4c5ef797b309"
      },
      "source": [
        "def show_confusion_matrix(confusion_matrix):\n",
        "  hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "  hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n",
        "  hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n",
        "  plt.ylabel('True sentiment')\n",
        "  plt.xlabel('Predicted sentiment');\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
        "show_confusion_matrix(df_cm)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z10_oAfUYjMX"
      },
      "source": [
        "### Check One Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnJHPe2MYlxk"
      },
      "source": [
        "idx = 2\n",
        "review_text = y_review_texts[idx]\n",
        "true_sentiment = y_test[idx]\n",
        "pred_df = pd.DataFrame({\n",
        "  'class_names': class_names,\n",
        "  'values': y_pred_probs[idx]\n",
        "})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_Tvsqq9Y7V0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "8e434c9d-55ca-44b0-87c5-b19516705f2c"
      },
      "source": [
        "from textwrap import wrap\n",
        "print(\"\\n\".join(wrap(review_text)))\n",
        "print()\n",
        "print(f'True sentiment: {class_names[true_sentiment]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I'm returning them.\n",
            "\n",
            "True sentiment: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqjP-yuIahMY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "8c9d8e38-0f08-44f2-b6a7-fa88baad789d"
      },
      "source": [
        "sns.barplot(x='values', y='class_names', data=pred_df, orient='h')\n",
        "plt.ylabel('sentiment')\n",
        "plt.xlabel('probability')\n",
        "plt.xlim([0, 1]);"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEGCAYAAABsLkJ6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAOxUlEQVR4nO3dfawmZX3G8e/FrouiWNHdNojgAYIviNaSlaI2vqC1vBgwLVqoVjFEo9a+qLWxqVZj+w+atglWQSy41YIvWGsXhZLGIrQEqIuIgohSigjSiFrRQioCv/4xs92nW9gzZ9mZ55xzfz/J5jzPzDwzv71zzrnOPffMPakqJEnt2W3eBUiS5sMAkKRGGQCS1CgDQJIaZQBIUqPWzruAWevXr6+FhYV5lyFJK8aVV175varasDOfXVYBsLCwwJYtW+ZdhiStGEm+tbOf9RSQJDXKAJCkRhkAktQoA0CSGmUASFKjDABJapQBIEmNMgAkqVEGgCQ1ygCQpEYZAJLUKANAkhplAEhSowwASWqUASBJjTIAJKlRBoAkNcoAkKRGGQCS1CgDQJIaZQBIUqMMAElqlAEgSY0yACSpUQaAJDXKAJCkRhkAktQoA0CSGmUASFKjDABJapQBIEmNMgAkqVEGgCQ1ygCQpEYZAJLUKANAkhplAEhSowwASWqUASBJjTIAJKlRBoAkNcoAkKRGGQCS1CgDQJIaZQBIUqMMAElqlAEgSY0yACSpUQaAJDXKAJCkRo0aAEmOTHJ9khuSvG3MY0mSlma0AEiyBng/cBRwMHBikoPHOp4kaWnG7AEcBtxQVTdW1d3Ax4HjRjyeJGkJxgyAfYBvz7y/pV/2fyR5bZItSbbcfvvtI5YjSZo190HgqjqjqjZW1cYNGzbMuxxJasaYAXArsO/M+8f1yyRJy8CYAfBF4KAk+ydZB5wAbB7xeJKkJVg71o6r6p4kbwQuBNYAZ1XVtWMdT5K0NKMFAEBVnQ+cP+YxJEk7Z+6DwJKk+TAAJKlRBoAkNcoAkKRGGQCS1CgDQJIaZQBIUqMMAElqlAEgSY0yACSpUQaAJDXKAJCkRhkAktQoA0CSGmUASFKjDABJapQBIEmNMgAkqVEGgCQ1ygCQpEYZAJLUKANAkhplAEhSowwASWqUASBJjRoUAEmePWSZJGnlGNoDeN/AZZKkFWLtjlYmeSbwLGBDkjfPrHoksGbMwiRJ49phAADrgEf02+05s/xHwPFjFSVJGt8OA6CqLgYuTrKpqr41UU2SpAks1gPYavckZwALs5+pqiPGKEqSNL6hAXAucDrwV8C945UjSZrK0AC4p6pOG7USSdKkhl4Gel6SNyTZO8mjt/4btTJJ0qiG9gBe1X9968yyAg7YteVIkqYyKACqav+xC5EkTWvoVBB7JHl7fyUQSQ5K8uJxS5MkjWnoGMCHgbvp7goGuBX401EqkiRNYmgAHFhV7wF+ClBVdwEZrSpJ0uiGBsDdSR5GN/BLkgOBn4xWlSRpdEOvAnon8A/AvknOBp4NnDRWUZKk8Q29Cugfk3wJOJzu1M/vVtX3Rq1MkjSqpTwRbB+6KaDXAc9J8qvjlCRJmsKgHkCSs4CnAdcC9/WLC/j0SHVJkkY2dAzg8Ko6eNRKJEmTGnoK6LIkBoAkrSJDewAfoQuB/6C7/DNAVdXTRqtMkjSqoQFwJvCbwFfZNgYgSVrBhgbA7VW1edRKJEmTGhoAVyU5BziPmTuAq8qrgCRphRoaAA+j+8X/opllXgYqSSvY0DuBXz12IZKkae0wAJL8QVW9J8n76CeCm1VVvzNaZZKkUS3WA7iu/7pl7EIkSdPaYQBU1Xn9y7uq6tzZdUleOlpVkqTRDb0T+A8HLpMkrRCLjQEcBRwN7JPk1JlVjwTuGbMwSdK4FhsD+A7d+f9jgStnlv8YeNNYRUmSxrfYGMDVwNVJzqmqn05UkyRpAkNvBDssybuAx/ef2ToZ3AG7spi7b7uWm9/91F25S0nSA1jKZHBvojsNdO945UiSpjI0AO6oqgtGrUSSNKmhAXBRkvfSzf0zOxncl0apSpI0uqEB8Iv9140zywo4YteWI0maytDJ4J4/diGSpGkNuhM4yc8lOTPJBf37g5OcPG5pkqQxDZ0KYhNwIfDY/v03gN8boyBJ0jSGBsD6qvok/fOAq+oevBxUkla0oQFwZ5LH0D8TIMnhwB2jVSVJGt3Qq4DeDGwGDkxyKbABOH60qiRJoxvaAzgQOAp4Ft1YwDcZHh6SpGVoaAC8o6p+BOwFPB/4AHDaaFVJkkY3NAC2DvgeA3yoqj4HrBunJEnSFIYGwK1JPgj8OnB+kt2X8FlJ0jI09Jf4y+jO/f9KVf0QeDTw1tGqkiSNbuhUEHfRTQS39f1twG1jFSVJGp+ncSSpUQaAJDXKAJCkRhkAktQoA0CSGmUASFKjDABJapQBIEmNMgAkqVEGgCQ1ygCQpEYZAJLUKANAkhplAEhSowwASWqUASBJjTIAJKlRBoAkNcoAkKRGGQCS1CgDQJIaZQBIUqMMAElq1GgBkOSsJN9Ncs1Yx5Ak7bwxewCbgCNH3L8k6UEYLQCq6hLgB2PtX5L04Mx9DCDJa5NsSbLlB3feO+9yJKkZcw+AqjqjqjZW1cZHP3zNvMuRpGbMPQAkSfNhAEhSo8a8DPRjwGXAE5PckuTksY4lSVq6tWPtuKpOHGvfkqQHz1NAktQoA0CSGmUASFKjDABJapQBIEmNMgAkqVEGgCQ1ygCQpEYZAJLUKANAkhplAEhSowwASWqUASBJjTIAJKlRBoAkNcoAkKRGGQCS1CgDQJIaZQBIUqMMAElqlAEgSY0yACSpUQaAJDXKAJCkRhkAktQoA0CSGmUASFKjDABJapQBIEmNMgAkqVEGgCQ1ygCQpEYZAJLUKANAkhplAEhSowwASWqUASBJjTIAJKlRa+ddwKx1ez+F/f54y7zLkKSV453Z6Y/aA5CkRhkAktQoA0CSGmUASFKjDABJapQBIEmNMgAkqVEGgCQ1ygCQpEYZAJLUqFTVvGv4X0l+DFw/7zqWifXA9+ZdxDJgO2xjW2xjW2zzxKrac2c+uKzmAgKur6qN8y5iOUiyxbawHWbZFtvYFtsk2ekJ1DwFJEmNMgAkqVHLLQDOmHcBy4ht0bEdtrEttrEtttnptlhWg8CSpOkstx6AJGkiBoAkNWryAEhyZJLrk9yQ5G33s373JJ/o11+RZGHqGqcyoC3enORrSb6S5PNJHj+POqewWFvMbPdrSSrJqr0EcEhbJHlZ/71xbZJzpq5xKgN+RvZLclGSq/qfk6PnUecUkpyV5LtJrnmA9Ulyat9WX0ly6KI7rarJ/gFrgH8DDgDWAVcDB2+3zRuA0/vXJwCfmLLGZdYWzwf26F+/vuW26LfbE7gEuBzYOO+65/h9cRBwFbBX//5n5133HNviDOD1/euDgZvmXfeI7fEc4FDgmgdYfzRwARDgcOCKxfY5dQ/gMOCGqrqxqu4GPg4ct902xwF/3b/+FPCCJDv/1OPla9G2qKqLququ/u3lwOMmrnEqQ74vAP4EOAX47ymLm9iQtngN8P6q+k+AqvruxDVOZUhbFPDI/vXPAN+ZsL5JVdUlwA92sMlxwEeqcznwqCR772ifUwfAPsC3Z97f0i+7322q6h7gDuAxk1Q3rSFtMetkunRfjRZti747u29VfW7KwuZgyPfFE4AnJLk0yeVJjpysumkNaYt3Aa9IcgtwPvDb05S2LC31d8qymwpC9yPJK4CNwHPnXcs8JNkN+HPgpDmXslyspTsN9Dy6XuElSZ5aVT+ca1XzcSKwqar+LMkzgY8mOaSq7pt3YSvB1D2AW4F9Z94/rl92v9skWUvXrfv+JNVNa0hbkOSFwB8Bx1bVTyaqbWqLtcWewCHAF5LcRHd+c/MqHQge8n1xC7C5qn5aVf8OfIMuEFabIW1xMvBJgKq6DHgo3URxLRr0O2XW1AHwReCgJPsnWUc3yLt5u202A6/qXx8P/FP1IxyrzKJtkeQXgA/S/fJfred5YZG2qKo7qmp9VS1U1QLdeMixVbXTk2AtY0N+Rj5D99c/SdbTnRK6ccoiJzKkLW4GXgCQ5Ml0AXD7pFUuH5uBV/ZXAx0O3FFVt+3oA5OeAqqqe5K8EbiQboT/rKq6Nsm7gS1VtRk4k64bdwPdgMcJU9Y4lYFt8V7gEcC5/Tj4zVV17NyKHsnAtmjCwLa4EHhRkq8B9wJvrapV10se2BZvAT6U5E10A8InrdI/GEnyMbrgX9+PebwTeAhAVZ1ONwZyNHADcBfw6kX3uUrbSpK0CO8ElqRGGQCS1CgDQJIaZQBIUqMMAElqlAGgpiX5ryVuvynJ8fezfGOSU/vXJyX5y/7165K8cmb5Y3dF3dKu4FQQWvWSrKmqe8c8Rn9T2v+7Ma2/Pnurk4BrWMUTlmllsQegFS3JQpKvJzk7yXVJPpVkjyQ3JTklyZeAlyY5MclXk1yT5JTt9vEX/bz6n0+yoV/2miRfTHJ1kr9NssfMR16YZEuSbyR5cb/985J89n7qe1eS3+97DRuBs5N8OckxST4zs90vJ/m7MdpIeiAGgFaDJwIfqKonAz+ie6YEwPer6lC6ZwicAhwBPB14RpKX9Ns8nO6u0qcAF9PdXQnw6ap6RlX9PHAd3ZwzWy3QTVV8DHB6kocuVmBVfYquh/Dyqno63V2bT9oaOHR3bZ615P+59CAYAFoNvl1Vl/av/wb4pf71J/qvzwC+UFW391OMn033cA2A+2a2m/3sIUn+OclXgZcDT5k53ier6r6q+ibdHDxPWmrB/XQFH6WbyvhRwDNZvdN9a5lyDECrwfbzmWx9f+eD2Ncm4CVVdXWSk+gnX1vkeEv1YeA8ugfcnNuHkzQZewBaDfbr54IH+A3gX7Zb/6/Ac5OsT7KGbg75i/t1u9HNOrv9Z/cEbkvyELoewKyXJtktyYF0jyu8fmCdP+73C0BVfYduQPjtdGEgTcoA0GpwPfBbSa4D9gJOm13ZT4n7NuAiuufKXllVf9+vvhM4rH/Q9hHAu/vl7wCuAC4Fvr7d8W6mC5ULgNdV1dBHVG6iGzP4cpKH9cvOpjuFdd3AfUi7jLOBakVLsgB8tqoOmXMpO6W/X+Cqqjpz3rWoPY4BSHOS5Eq6Hshb5l2L2mQPQJIa5RiAJDXKAJCkRhkAktQoA0CSGmUASFKj/gf5fWutap8TMQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiGiAv_Datli"
      },
      "source": [
        "### Preding Raw Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34LA3twZawMe"
      },
      "source": [
        "review_text = \"DSP is a great course!\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrNutK4gbFUv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "35c068ee-7ea4-48db-9a53-aac198e5e92d"
      },
      "source": [
        "encoded_review = tokenizer.encode_plus(\n",
        "  review_text,\n",
        "  max_length=MAX_LEN,\n",
        "  add_special_tokens=True,\n",
        "  return_token_type_ids=False,\n",
        "  pad_to_max_length=True,\n",
        "  return_attention_mask=True,\n",
        "  return_tensors='pt',\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjOutwoMbMf3"
      },
      "source": [
        "input_ids = encoded_review['input_ids'].to(device)\n",
        "attention_mask = encoded_review['attention_mask'].to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iz94v7mbbSIb"
      },
      "source": [
        "output = model(input_ids, attention_mask)\n",
        "_, prediction = torch.max(output, dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qaGtKXkbf7P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3f797864-ef2d-4422-bb9d-12f39cdd43c9"
      },
      "source": [
        "print(f'Review text: {review_text}')\n",
        "print(f'Sentiment  : {class_names[prediction]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Review text: DSP is a great course!\n",
            "Sentiment  : 1\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}